{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent to use this jupyter for myself, quick refresher / class-notes for course material studied for  Neural Networks from Udacity [ & not to reproduce any material as such ..]\n",
    "\n",
    "This guide DOES NOT go in depth explaining fundamentals, & may appear be in arbitraty sequence to follow, as it is aligned to class labs. One should go through proper course material prior using this guide. I highly recommend Udacity nanodegree course(s) as right medium to learn this subject.\n",
    "\n",
    "Content Credit: Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs).\n",
    "\n",
    "The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous(also known as **temporal**) inputs when generating the current output. In other words, our systems did not have any **memory** elements. RNNs address this very basic and important issue by using **memory** (i.e. past inputs to the network) when producing the current output.\n",
    "\n",
    "** RECURRENT ** - Occuring often or reapeatedly\n",
    "\n",
    "So what do we call these networks RNN, it is because we perform the same task for each element in the input sequence.\n",
    "\n",
    "RNN are similar to feed-forward networks, let have a quick refresher of feed-forward network..\n",
    "\n",
    "<img src=\"images/rnn1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## Before we go deep, let's study bit of history of how RNN evolved ..\n",
    "\n",
    "After the first wave of **FFNN (Feed Forward Neural Networks) in 1980s**, it was clear that they have limilation as they are unable to handle temporal dependencies, which as we mentioned above that dependecies which change over time.\n",
    "\n",
    "<img src=\"images/rnn2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** Modeling temporal data is critical in most real-world applications, since natural signals like speech and video have time variant properties & are characterized by having dependencies across time. **\n",
    "\n",
    "Btw, Biologocal Neural Networks have recurring connections, hence, applying to recurrence to artificial feedfoward neural networks made natural sense.\n",
    "\n",
    "**First Attempt** to add memory to Neural Networks were the **Time Delay Neural Networks, TDNNs in short **. In TDNNS, inputs from past timesteps were introduced to the Neural Input, changing the actual external inputs. This had the advantage to have network look beyond current time step, but also introduced the clear disadvantage, since the temporal dependecies were limited to the window of time chosen.\n",
    "\n",
    "<img src=\"images/rnn3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** Simple RNN & ELMON NETWORK (1990)** were next to follow..  \n",
    "\n",
    "<img src=\"images/rnn4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "It was recognised that all these networks in early 90s were suffered from what we call, ** Vanishing Gradient problem **, in which contributions of information decayed geometrically over time.. Hence,\n",
    "capturing relationships that spanned more than eight or ten steps back was practically impossible.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In mid 90s ** Long Short-Term Memory (LSTM) ** Cells were invented to tackle vanishing gradient, susequently fundamental issue of adding memory to these networks.\n",
    "\n",
    "<img src=\"images/rnn6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The novelty in LSTMs was the idea that some , what we called state variables, can be kept fixed using gates & re-introduced or not at an appropriate time in future. In this way, arbitraty time intervals can be represented, and temporal dependencies can be captured.\n",
    "\n",
    "Variations in LSTM such as, ** Gated Recurrent Networks (GRUs) ** further refined this there and nowadays represent another mainstream approach to realizing RNNs.\n",
    "\n",
    "\n",
    "*************\n",
    "\n",
    "** Optional Read: **\n",
    "\n",
    "1. What does Vanishing Gradient mean??\n",
    "\n",
    "While training our network we use backpropagation. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\"\n",
    "\n",
    "detailed read..\n",
    "https://en.wikipedia.org/wiki/Vanishing_gradient_problem\n",
    "\n",
    "Geometric Series and how its values may exponentaially decrease..\n",
    "https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions\n",
    "\n",
    "2. If you are still curious, for more information on the important milestones mentioned here, please take a peek at the following links:\n",
    "\n",
    "**TDNN** https://en.wikipedia.org/wiki/Time_delay_neural_network\n",
    "\n",
    "** Original ELMAN Network Publication of 1990 ** http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract . To simplify this, this link can be studied as well https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks\n",
    "\n",
    "**LSTM** http://www.bioinf.jku.at/publications/older/2604.pdf -- In this LSTM link you will find the original paper written by Sepp Hochreiter and Jürgen Schmidhuber\n",
    "\n",
    "**GRUs** https://deeplearning4j.org/lstm.html  (begineer Guide to RNN & LSTMs )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are key applications of RNN?\n",
    "\n",
    "1. Speech Recognition : \n",
    ">Google Assistant <br>\n",
    ">Amazon Alexa<br>\n",
    ">Apple Siri<br>\n",
    ">Nuances Dragon<br>\n",
    "\n",
    "2. Time Series Predictions: \n",
    "> Traffic Patterns : Waze<br>\n",
    "> Movie Selection : Netflix <br>\n",
    "> Stock Movement : Hedge Funds<br>\n",
    "\n",
    "3. NLP\n",
    "> Machine Translation : Google, Salesforce<br>\n",
    "> Q&A : Google Analytics<br>\n",
    "> Chatbots : Slack, Google, Baidu<br>\n",
    "\n",
    "4. Gesture Recognition\n",
    "\n",
    "5. Are you into gaming and bots?\n",
    "> DotA 2 bot by Open AI : https://blog.openai.com/dota-2/\n",
    "\n",
    "6. How about automatically adding sounds to silent movies? https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8\n",
    "\n",
    "7. Here is a cool tool for automatic handwriting generation. http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3\n",
    "\n",
    "8. Amazon's voice to text using high quality speech recognition, Amazon Lex. https://aws.amazon.com/lex/faqs/\n",
    "\n",
    "9. Facebook uses RNN and LSTM technologies for building language models. https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/\n",
    "\n",
    "10. Netflix also uses RNN models - here is an interesting read. https://arxiv.org/pdf/1511.06939.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Requisite\n",
    "\n",
    "I will skip over details here, see NN in Nut Shell Jupyter, below is quick summary of what one should know before hand .. \n",
    "\n",
    "** Feed-Forward NN (FFNN) **\n",
    "\n",
    "<img src=\"images/rnn7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Quick Refresher on **Activation Functions**: https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions\n",
    "\n",
    "**tanh** = -1 to 1 <br>\n",
    "**sigmoid** = 0 to 1 <br>\n",
    "**ReLU** = Work liek switch, turn -ve values to zero & +ve values remains as they are.<br>\n",
    "(Each have they advantages and dis-advantages)\n",
    "\n",
    "<img src=\"images/rnn9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Error Functions:** The two error functions that are most commonly used are the **Mean Squared Error (MSE)** (usually used in regression problems & also called **Log Function**) and the **Cross Entropy** (usually used in classification problems & also called **Log Loss Function**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs and RNNs can be combined\n",
    "\n",
    "CNN can be used to extract features & RNN can be used for memory injection, popular application gesture recognition.\n",
    "\n",
    "<img src=\"images/rnn8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Backpropagation **\n",
    "\n",
    "**Derivatives: ** http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html\n",
    "\n",
    "**Common Derivates Cheat Sheet : **http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf\n",
    "\n",
    "** Tuning Learning Rate in Gradient Descent ** : http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/#loss\n",
    "\n",
    "\n",
    "** Overfitting ** \n",
    "> Mitigations\n",
    "> 1. Stopping Early\n",
    "> 2. Regularization (Dropout)\n",
    "\n",
    "**Mini-Batching** Updating the weights every N Steps\n",
    "> Reduction of the complexity if the training process\n",
    "> Noise Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have refreshed all key subjects before and are ready to start RNNs now .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we know thus far is RNNs can maintain previous inputs by maintaining internal memory elements, also known as **States**\n",
    "\n",
    "<img src=\"images/rnn10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Many applications have temporal dependencies, which means Output they just do not depend on Current Input but also takes in to account Past Inputs. For cases like these we need to use RNNs\n",
    "\n",
    "<img src=\"images/rnn11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "A good and simple example for RNN could be 'Predicting Next Word in the Sentence', which typically requires looking at the last few words rather than current one\n",
    "\n",
    "<img src=\"images/rnn12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "And we already discussed some other relevant areas of study where RNN is apt to be used .. and frankly use cases of RNN are just popping up everywhere and hard to keep up with ..\n",
    "\n",
    "<img src=\"images/rnn13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "RNN are very similar to FFNN, they have very similar input and output patterns like, many-to-many, many-to-one, one-to-many.\n",
    "\n",
    "However RNNs have 2 fundamental differences from FFNNs, which are:\n",
    "\n",
    "**1st Difference: Seuqences as Inputs** - Instead of training network with single input, single output at each time step, we train with sequences since previous input matters.\n",
    "\n",
    "**2nd Difference: Memeory Elements** - Stems from the memory elements RNNs host, current inputs, as well as activations of neurons serve as inputs to the next time step.\n",
    "\n",
    "<img src=\"images/rnn14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In FFNN we saw flow from input to the outout w/o any feedback ..\n",
    "\n",
    "<img src=\"images/rnn15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Now, the FF scheme changes & includes feedback or memory elements. We will define memory as the output of the hidden layer, which will serve as an additinal input to the network at the following training step.\n",
    "\n",
    "<img src=\"images/rnn16.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We will no longer use H as the output of the hidden layer, but S as state referring to system with memory.\n",
    "\n",
    "<img src=\"images/rnn17.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The basic scheme of RNN or on other words basic three layer neural network with feedback that serve as memory inputs is called  **Simple RNN or Elman Network**.\n",
    "Not that above pictire depicts nth outputs which is feasible, but we will use 2 outputs to understand foundation better and prior looking at complex networks ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So let's dwell bit deep on how RNN works .. \n",
    "\n",
    "As we discussed earlier that RNNs have unique way of injecting States ( aka memort units) in to the network. Let's see how can this be implemented.\n",
    "\n",
    "As we've see, in FFNN the output at any time t, is a function of the current input and the weights. This can be easily expressed using the following equation:\n",
    "\n",
    "y_hat = F($x_{t}$,W)\n",
    "\n",
    "In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:\n",
    "\n",
    "y_hat = F ($x_{t}$, $x_{t-1}$, $x_{t-2}$, ..., $x_{t-t0}$,W)\n",
    "\n",
    "This is called RNN ** Folded Model **.\n",
    "\n",
    "<img src=\"images/rnn18.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "The model can also be \"unfolded in time\". The unfolded model is usually what we use when working with RNNs.\n",
    "\n",
    "<img src=\"images/rnn19.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In both the folded and unfolded models shown above the following notation is used:\n",
    "\n",
    "x represents the input vector, <br>\n",
    "y represents the output vector and <br>\n",
    "s denotes the state vector.\n",
    "\n",
    "$W_{x}$ is the weight matrix connecting the inputs to the state layer.\n",
    "\n",
    "$W_{y}$ is the weight matrix connecting the state layer to the output layer.\n",
    "\n",
    "$W_{s}$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function \\PhiΦ in the following way:\n",
    "\n",
    "h = Φ(x W)\n",
    "\n",
    "In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and also on the previous state:\n",
    "\n",
    "<img src=\"images/rnn20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "** RNNs can be stacked like Lego's and can derive more interesting outcomes, as in practice, it does not matter where inputs comes from and what outputs we are headed too ..**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "We are now ready to understand how to train the RNN.\n",
    "\n",
    "When we train RNNs we also use backpropagation, but with a conceptual change. The process is similar to that in the FFNN, with the exception that we need to consider previous time steps, as the system has memory. This process is called ** Backpropagation Through Time (BPTT) **\n",
    "\n",
    "<img src=\"images/rnn21.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now unfold the model. You will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time process. These multiplications stem from the chain rule and are easily visualized using this model.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn22.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn23.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn24.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step! Adjusting $W{_x}$, the weight matrix connecting the state to the output.\n",
    "\n",
    "<img src=\"images/rnn25.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn26.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn27.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn28.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn29.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen Foward Pass & BPTT - we are ready to train, as reminder, we can follow **Mini Batching** concept here, i.e. we compute gradient at every step, but choose to update weights once every N Steps as shown below ..\n",
    "\n",
    "<img src=\"images/rnn30.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### What happens if we have many time steps, can we simply accumulate the gradient contributions from each of these steps ?? Answer is NO\n",
    "\n",
    "<img src=\"images/rnn31.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Studies have shown that fir upto a small number of time steps say 8 or 10, we can effectively used this method. Gimg beyond that we Gradient wukk become to osmall which is known as ** Vanishing Gradient Problem **. IN other words, temporal dependencies over 8 or 9 time steps will discarded by the network. \n",
    "\n",
    "### So how to solve Vanishing Gradient Problem ??\n",
    "\n",
    "LSTM - Long Short-Term memory Cells were invented specifically to address this problem and we will study this in detail ahead ..\n",
    "\n",
    "<img src=\"images/rnn32.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Exploding Gradients is another issue we should be aware ..\n",
    "\n",
    "In this gradient grows uncontrollably, luckily a simple scheme called **Gradient Clipping** resolves this issue.\n",
    "\n",
    "<img src=\"images/rnn33.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Basically, we check if Gradient cross certain threshhold, if it does, we normalize (penalize) excessively large gradients crossing thresholds, and this helps solve Exploding Gradient problem.\n",
    "\n",
    "<img src=\"images/rnn34.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "In depth article on Gradient Clipping: https://arxiv.org/abs/1211.5063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs\n",
    "\n",
    "**Long Short-Term Memory Cells, (LSTM)** give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies. They were proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber\n",
    "\n",
    "If we take a close look at the RNN neuron, we can see that we have simple linear combinations (with or without the use of an activation function). We can also see that we have a single addition.\n",
    "\n",
    "Zooming in on the neuron, we can graphically see this in the following configuration:\n",
    "\n",
    "<img src=\"images/rnn35.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The **LSTM** cell is a bit more complicated. If we zoom in on the cell, we can see that the mathematical configuration is the following:\n",
    "\n",
    "<img src=\"images/rnn36.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The LSTM cell allows a recurrent system to learn over many time steps without the fear of losing information due to the vanishing gradient problem. It is fully differentiable, therefore gives us the option of easily using backpropagation when updating the weights.\n",
    "\n",
    "LSTMs have 4 different functions whcih are computed in 3 steps as listed below .. these help us retain \n",
    "\n",
    "<img src=\"images/rnn37.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Study Material for RNNs:\n",
    "\n",
    "Chris Olah's Post:\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Edwin Chen's Post:\n",
    "http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "Andrej Karpathy's Lecture:\n",
    "https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Study LSTM with some examples...\n",
    "\n",
    "**Step 1** So we have NN below which receives input as image and predicts this as Dog\n",
    "\n",
    "<img src=\"images/rnn38.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Step 2** How do we know for sure that it is Dog and not Wolf ?\n",
    "\n",
    "This is where RNN comes in picture, if we have **memory** like show in example below, that we have been seeing forest animals, our output would most likely be inclined towards Wolf instead of Dog isn't it ??\n",
    "\n",
    "<img src=\"images/rnn39.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "** Step 3 ** What if the memory is not immediate (Short Term) and of recent past like this (i.e. Long Term)?? In case of RNNs, because of Vanishing Gradient, we will not be able to take advantage of that, instead immediate memory States (Short Term) will influence our decision and predict the outcome most likely as Dog instead .. \n",
    "\n",
    "<img src=\"images/rnn40.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "This is where **LSTM** comes in as it can keep track of both Short Term and Long Term memories.\n",
    "\n",
    "Quick summary of approach and differences between RNN and LSTMs\n",
    "\n",
    "**RNN** Memory + Current Input combined derive two outcomes,first prediction, second, new memory whcih act as input to next State.\n",
    "\n",
    "<img src=\"images/rnn41.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**LSTM** LSTMs are very similar, with difference that it keep track of two memories ** Long Term and Short Term **, so the present state depends on 3 inputs instead, i.e. Long Term Memory, Short Term Memory and Current Input all these 3 inputs gets merged to derive 3 outputs, prediction, and new Short Term + Long Term Memory.\n",
    "\n",
    "<img src=\"images/rnn42.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of RNN vs LSTM\n",
    "\n",
    "We have studied RNN at depth and hence will not go over any details here ..\n",
    "<img src=\"images/rnn43.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "LSTMs architecture looks very complicated with many functions inside, but it is not as complex as it looks, let's look at it in detail ahead ..\n",
    "\n",
    "<img src=\"images/rnn44.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gates of LSTM \n",
    "\n",
    "There are four main gates .. \n",
    "1. Learn Gate \n",
    "2. Forget Gate \n",
    "3. Remember Gate\n",
    "4. Use Gate\n",
    "\n",
    "Let's study how these gates work .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learn Gate \n",
    "Let's look at memories we have on the given example ...\n",
    "\n",
    "<img src=\"images/rnn45.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Learn Gate takes these inputs and combine them .. \n",
    "\n",
    "<img src=\"images/rnn46.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Actually it does more than combining them ..\n",
    "\n",
    "Do first it combine inputs .. \n",
    "\n",
    "<img src=\"images/rnn47.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "And then ignores the non-relevant part, in this case, Tree is non-relevant, so it ignores the tree .. \n",
    "\n",
    "<img src=\"images/rnn48.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "And subseuqently the final output as ..\n",
    "\n",
    "<img src=\"images/rnn49.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "** So how does this work Mathematically **\n",
    "\n",
    "**Combine** As we have seen before, It takes Input, Short Term Memory combone them using activation (in this example tanh) ..\n",
    "\n",
    "<img src=\"images/rnn50.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Ignore**\n",
    "\n",
    "We forget **Multiplying** by **Ignore Factor $i{_t}$ **, it is actually a vector byt get multiplied elementwise. So how do we compute $i{_t}$ **? So we take previous information of out short term memory & the event, creates a small NN, passing them through small linear function with a **New matrix $W{_i}$ & Bias **, squish them with sigmoid function to keep it between zero and one .. and that is it, this is how learn gate works .. \n",
    "\n",
    "<img src=\"images/rnn51.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forget Gate \n",
    "\n",
    "Forget takes the input as Long Term Memory and try to compute what to forget ..\n",
    "\n",
    "<img src=\"images/rnn52.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So in this example, we have memories of Nature and Science and Forget Gate decide to Keep Nature and Forget Science ..\n",
    "\n",
    "<img src=\"images/rnn53.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So how does it work mathematically .. ??\n",
    "\n",
    "It takes **Long Term Meory State** and **Multiplies** the same with **Forget Factor**, and we compute **Forget Factor** as before, by creating small NN, which takes **Short Term Memory** and **Input**, linearly combine them to compute **Forget Factor**.\n",
    "\n",
    "<img src=\"images/rnn54.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remember Gate \n",
    "\n",
    "It combines LTM coming out of Forget Gate && STM coming out of Learn Gate and simply combine them.\n",
    "\n",
    "<img src=\"images/rnn55.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Mathematically it is simple **Addition Function** \n",
    "\n",
    "<img src=\"images/rnn56.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Gate (aka Output Gate)\n",
    "\n",
    "Use Gate uses LTM coming out of Forget Gate && STM coming out of Learn Gate and simply combine keeping what is relevant to produce output as ** New Short Term Memory **.. In this case, it takes bear from LTM and Squirrel from STM along with Input as new STM ..\n",
    "\n",
    "** Note Image below has Error, Output is shown as New LTM instead of STM **\n",
    "\n",
    "<img src=\"images/rnn57.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Mathemtaically, it applies tanh on  LTM coming out of Forget Gate && Sigmoid on STM coming out of Learn Gate, further Multiply them to compute New STM\n",
    "\n",
    "<img src=\"images/rnn58.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together ...\n",
    "\n",
    "<img src=\"images/rnn59.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn60.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So there may be curiosity that this is all abritary, some places we use tanh, some sigmoid, multiplication, addition etc. IN fact it is arbitraty however has been proven to work, one can experiment, this area is very much under development + research.\n",
    "\n",
    "Reference for Detailed Study : https://deeplearning4j.org/lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Architectures that work ..\n",
    "\n",
    "**GRU - Gated Recurrent Unit** It combines Forget & Learn gate into Update Gate & runs this through Combine Gate. It also working with only one Working Memory insteas of LTM and STM - but in practce this works very well.\n",
    "\n",
    "<img src=\"images/rnn61.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Peephole Connections:** ie using LTM as well to calculate forget factor ..\n",
    "<img src=\"images/rnn62.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "<img src=\"images/rnn63.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "\n",
    "Detailed References to Study:\n",
    "\n",
    "Michael Guerzhoy's post\n",
    "http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf\n",
    "\n",
    "Steve Carell\n",
    "http://despicableme.wikia.com/wiki/Felonius_Gru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-wise RNN\n",
    "\n",
    "This network will learn about some text one character at a time & then generate new text one character at a time.\n",
    "\n",
    "** Batching ** we stuied earlier is the one way we can optimize compute and even performance is handled well. However, it could be confusing for RNNs, it is more of a programming challenge than anything to do with Deep Learning.\n",
    "\n",
    "Let's have quick look at Batching Fundamentals again ..\n",
    "\n",
    "Below is the sequence: <br>\n",
    "\n",
    "[ 1 2 3 4 5 6 7 8 9 10]\n",
    "\n",
    "We can pass this as-is to RNN however it will be better if we can pass this in two sequences, or what we also called as **Batches** like below :<br>\n",
    "\n",
    "Batch 1 : [1 2 3 4 5 ] <br>\n",
    "Batch 2 : [6 7 8 9 10 ]\n",
    "\n",
    "We can also decide on length of sequence in addition to batches, example we could say the length is 2 ..so the out batches will split as below ..\n",
    "\n",
    "So with length 2, the batch of data we will pass to the nrtwork would be:\n",
    "\n",
    "Batch 1 : [1 2] <br>\n",
    "Batch 2 : [6 7]\n",
    "\n",
    "Second Batch,\n",
    "\n",
    "Batch 1 : [3 4] <br>\n",
    "Batch 2 : [8 9]\n",
    "\n",
    "Third batch or until we run out of data ..\n",
    "\n",
    "Batch 1 : [5] <br>\n",
    "Batch 2 : [10]\n",
    "\n",
    "Note, the last batch is just one element each, as it does not have enough values to meet defined sequence length ..\n",
    "\n",
    "We can rertain the hidden State from one batch and use that as start of next batch, and this way we can pass information between batches ...\n",
    "\n",
    "With these concepts clear ... let's dive in to the code.\n",
    "\n",
    "Look at this git-hub repo for detailed lab on character-wise-rnn jupyter: https://github.com/anshoomehra/udacity-deep-learning/tree/master/intro-to-rnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters\n",
    "\n",
    "There is no standard hyper-parameters which works for all cases unfortunately. Hence we have to look for indicators to fine tune them.\n",
    "\n",
    "We can broadly bucket these in two catergories ..\n",
    "\n",
    "1. Optimizer Parameters\n",
    "> 1.1 Learning Rate <br>\n",
    "> 1.2 Mini Batch Size<br>\n",
    "> 1.3 Number of training iterations or epochs<br>\n",
    "\n",
    "2. Model Hyperparameters\n",
    "> 2.1 Number of layers & hideen units <br>\n",
    "> 2.2 Model specific parameters for architetures like RNNs<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Learning Rate\n",
    "\n",
    "This is one of the important hyper-parameters. If we have normalized the inputs .. the good **starting point is 0.01** and below are other suspects to try if this does not perform well ..\n",
    "\n",
    "0.1<br>\n",
    "0.01<br>\n",
    "0.001<br>\n",
    "0.0001<br>\n",
    "0.00001<br>\n",
    "0.000001<br>\n",
    "\n",
    "So how do we know which one to try ??\n",
    "\n",
    "Simple intuitions ..\n",
    "\n",
    "<img src=\"images/rnn64.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "We know though that in real world, error curve may not be U shaped rather it could much complex non-linear projection with n-dimesions to it, a simple visual to put point across below ..\n",
    "\n",
    "<img src=\"images/rnn65.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "One of the 'sort of automated' ways to handle this complexity could be use the technique called ** Learning Decay ** which as name implies is the gradual decrease of learning rate based on certain factors, some examples below ..\n",
    "\n",
    "1. decrease it by half-every 5 epochs\n",
    "<img src=\"images/rnn66.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "2. Decrease exponentially, example multiply LR with .01 every 8 epochs ..\n",
    "<img src=\"images/rnn67.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "3. Adaptive Learning Rate: Not always static decay work and even at times we need to increase LR, so adpative is possibly best way, understanding indicators and adjusting the LR.\n",
    "\n",
    "Detailed read on this subject ..\n",
    "\n",
    "Tensorflow Exponential decay: https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay\n",
    "\n",
    "Adaptive Learning Optimizers\n",
    "AdamOptimizer : https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "AdagradOptimizer : https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Minibatch Size\n",
    "\n",
    "1. **Online / Stochastic **: Is divide inputs into smaller sets, and run the network on each set.\n",
    "2. **Batch** : Send all inputs / entrire dataset and run neywork on this in one go ...\n",
    "\n",
    "<img src=\"images/rnn68.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "3. ** Mini-Batch**: Abstraction commonly used is to set Mini-Batch. This may sound confusing with Online, so online would be when the batch size is one, minibatch would be when batch size is between 1 and n (n is number of batches)\n",
    "\n",
    "<img src=\"images/rnn69.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "**So what should be the idal minibatch size .. ?**\n",
    "\n",
    "Values between 1, 2, 4, 8, 16, 32, 64, 128, 256 could be good range, and ideally 32 could be good start.\n",
    "\n",
    "Large Batch sizes may help with computational boost, but it would come at the cost of huge memory required, and more computational resources.\n",
    "\n",
    "In practice, Small Minibatches have more noise, but this noise can help AVIOD network at local minima, where as Large Minibatches may have higher tendencay to get stuck at local minima.\n",
    "\n",
    "Below is the study on CNNs (changing batch and no changes to LR). It shows that accuracy of model decreases larger the batch size becomes ..which states that learning rate has to be adjusted along, and subsequent image shows that network accuracy does go down, but only slightly when batch size increase is coupled with learning rate adjustment .. \n",
    "\n",
    "<img src=\"images/rnn70.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn71.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So in summary, \n",
    "1. Too small batch could be too slow\n",
    "2. Too large could be computatinally taxing and could impact accuracy \n",
    "3. Intermiate values as shown below could be good starting and experimental sizings .. \n",
    "\n",
    "<img src=\"images/rnn72.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Number of Epochs / Training Runs / Iterations\n",
    "\n",
    "The key metrics here is the **validation error**, ideally runs epochs until the validation error is decreasing, and stop when the error rate stops to go down. There is also a technique named **Early Stopping**, i.e. stop the network process when at certain threshold error stop to go down, say if last 4 epochs has no significant change in error rate, stops the process.\n",
    "\n",
    "<img src=\"images/rnn73.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "** Tensorflow has support for this **\n",
    "**ValidationMonitor (Deprecated)**\n",
    "In tensorflow, we can use a [ValidationMonitor with tf.contrib.learn](https://www.tensorflow.org/get_started/monitors#early_stopping_with_validationmonitor) to not only monitor the progress of training, but to also stop the training when certain conditions are met.\n",
    "\n",
    "The following example from the ValidationMonitor documentation shows how to set it up. Note that the last three parameters indicate which metric we're optimizing.\n",
    "\n",
    "      validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "      test_set.data,\n",
    "      test_set.target,\n",
    "      every_n_steps=50,\n",
    "      metrics=validation_metrics,\n",
    "      early_stopping_metric=\"loss\",\n",
    "      early_stopping_metric_minimize=True,\n",
    "      early_stopping_rounds=200)\n",
    "      \n",
    "The last parameter indicates to ValidationMonitor that it should stop the training process if the loss did not decrease in 200 steps (rounds) of training.\n",
    "\n",
    "The validation_monitor is then passed to tf.contrib.learn's \"fit\" method which runs the training process:\n",
    "\n",
    "    classifier = tf.contrib.learn.DNNClassifier(\n",
    "      feature_columns=feature_columns,\n",
    "      hidden_units=[10, 20, 10],\n",
    "      n_classes=3,\n",
    "      model_dir=\"/tmp/iris_model\",\n",
    "      config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\n",
    "\n",
    "    classifier.fit(x=training_set.data,\n",
    "               y=training_set.target,\n",
    "               steps=2000,\n",
    "               monitors=[validation_monitor])\n",
    "               \n",
    "**SessionRunHook**\n",
    "More recent versions of TensorFlow deprecated monitors in favor of [SessionRunHooks](https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook). SessionRunHooks are an evolving part of tf.train, and going forward appear to be the proper place where you'd implement early stopping.\n",
    "\n",
    "At the time of writing, two pre-defined stopping monitors exist as a part of tf.train's [training hooks](https://www.tensorflow.org/api_guides/python/train#Training_Hooks):\n",
    "\n",
    "[StopAtStepHook](https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook): A monitor to request the training stop after a certain number of steps\n",
    "[NanTensorHook](https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook): a monitor that monitor's loss and stops training if it encounters a NaN loss\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### Number of Hidden Unit Layers\n",
    "\n",
    "This parameter is tricky, in general more the number of hidden layers / units more the network can learn. How much 'more' is enough as though it can hel improve accuracy, it also has direct cost in terms if memory and computations. \n",
    "\n",
    "So one rule of thumb could be to observe Training and Validation Accuracy Rate, if accuracy of training is way higher than Validation, it states model is **Overfitting** - 1. We could lower the layers/units to solve this\n",
    "2. We could also do use the Dropout, L2 Regularization \n",
    "\n",
    "<img src=\"images/rnn74.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "<img src=\"images/rnn75.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "** Under-fitting** Not enough juice, either input data is too less or number of hidden units & layers are not adequate enough, increasing these may help .\n",
    "\n",
    "** TIPS : Few Heuristics (from what worked well in most scenarios) **\n",
    "1. Hidden Units more than Inputs Units tends to perform better \n",
    "2. 3 Layer Architecture perform better than 2 or 1, however more than 3 layers mostly has not help  much .. Only exception is CNNs, the deepr they are the better in terms of performance..\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "### RNN Hyperparameters\n",
    "\n",
    "**Three Main Choices:**\n",
    "\n",
    "1. Choice of Cells Vanilla vs LSTM vs GRU \n",
    "2. How many layers / units we should stack..\n",
    "3. If our inputs are words, we should also look at words embedding/dimensionality \n",
    "\n",
    "\n",
    "**Cells Vanilla vs LSTM vs GRU**\n",
    "<img src=\"images/rnn76.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "There has been no clear winner, if you see couple research outcomes below, in one case GRU is tad better, in other, LSTM - GRU tie and beat each other on different batch size. In general LSTM is more popular.\n",
    "\n",
    "<img src=\"images/rnn77.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "<img src=\"images/rnn78.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "\"These results clearly indicate the advantages of the gating units over the more traditional recurrent units. Convergence is often faster, and the final solutions tend to be better. However, our results are not conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of gated recurrent unit may depend heavily on the dataset and corresponding task.\"\n",
    "\n",
    "[Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](https://arxiv.org/abs/1412.3555) by Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio\n",
    "\n",
    "\"The GRU outperformed the LSTM on all tasks with the exception of language modelling\"\n",
    "\n",
    "[An Empirical Exploration of Recurrent Network Architectures](http://proceedings.mlr.press/v37/jozefowicz15.pdf) by Rafal Jozefowicz, Wojciech Zaremba, Ilya Sutskev\n",
    "\n",
    "** How many layers / units we should stack.. **\n",
    "\"Our consistent finding is that depth of at least two is beneficial. However, between two and three layers our results are mixed. Additionally, the results are mixed between the LSTM and the GRU, but both significantly outperform the RNN.\"\n",
    "\n",
    "[Visualizing and Understanding Recurrent Networks](https://arxiv.org/abs/1506.02078) by Andrej Karpathy, Justin Johnson, Li Fei-Fei\n",
    "\n",
    "\"Which of these variants is best? Do the differences matter? [Greff, et al. (2015)](https://arxiv.org/pdf/1503.04069.pdf) do a nice comparison of popular variants, finding that they’re all about the same. [Jozefowicz, et al. (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.\"\n",
    "\n",
    "[Understanding LSTM Networks by Chris Olah](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\"In our [Neural Machine Translation] experiments, LSTM cells consistently outperformed GRU cells. Since the computational bottleneck in our architecture is the softmax operation we did not observe large difference in training speed between LSTM and GRU cells. Somewhat to our surprise, we found that the vanilla decoder is unable to learn nearly as well as the gated variant.\"\n",
    "\n",
    "[Massive Exploration of Neural Machine Translation Architectures](https://arxiv.org/abs/1703.03906v2) by Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc Le\n",
    "\n",
    "<img src=\"images/rnn79.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "However, for speech recognition, 5 to 7 layernetwork has performed well ..and even w/o LSTMs\n",
    "\n",
    "<img src=\"images/rnn80.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "** Words Embedding / Dimensionality **\n",
    "\n",
    "Some tasks improve larger we making the embedding ... at least a size of 200\n",
    "\n",
    "<img src=\"images/rnn81.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "However, in other study only marginal improvement is realized beyond the size of 50\n",
    "\n",
    "<img src=\"images/rnn82.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "<img src=\"images/rnn83.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "Reference Papers Below for above  table (starting row 1):\n",
    "Speech Recognition: https://arxiv.org/abs/1610.09975\n",
    "Speech Recognition: https://arxiv.org/abs/1303.5778\n",
    "Machine Translation: https://arxiv.org/abs/1409.3215\n",
    "Image Captioning: https://arxiv.org/abs/1411.4555\n",
    "Image Generation: https://arxiv.org/abs/1502.04623\n",
    "Question Answering: http://www.aclweb.org/anthology/P15-\n",
    "Text Summarization: https://pdfs.semanticscholar.org/3fbc/45152f20403266b02c4c2adab26fb367522d.pdf\n",
    "\n",
    "\n",
    "If you want to learn more about hyperparameters, these are some great resources on the topic:\n",
    "<br>\n",
    "Practical recommendations for gradient-based training of deep architectures by Yoshua Bengio - https://arxiv.org/abs/1206.5533\n",
    "<br>\n",
    "\n",
    "Deep Learning book - chapter 11.4: Selecting Hyperparameters by Ian Goodfellow, Yoshua Bengio, Aaron Courville - http://www.deeplearningbook.org/contents/guidelines.html\n",
    "<br>\n",
    "Neural Networks and Deep Learning book - Chapter 3: How to choose a neural network's hyper-parameters? by Michael Nielsen - http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters\n",
    "<br>\n",
    "Efficient BackProp (pdf) by Yann LeCun - http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n",
    "<br>\n",
    "More specialized sources:\n",
    "<br>\n",
    "How to Generate a Good Word Embedding? by Siwei Lai, Kang Liu, Liheng Xu, Jun Zhao - https://arxiv.org/abs/1507.05523\n",
    "<br>\n",
    "Systematic evaluation of CNN advances on the ImageNet by Dmytro Mishkin, Nikolay Sergievskiy, Jiri Matas - https://arxiv.org/abs/1606.02228\n",
    "<br>\n",
    "Visualizing and Understanding Recurrent Networks by Andrej Karpathy, Justin Johnson, Li Fei-Fei - https://arxiv.org/abs/1506.02078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings & Word2Vec\n",
    "\n",
    "Word Embeddings\n",
    "\n",
    "This is a deep neural network method for representing data with a huge number of classes more efficiently. Embeddings greatly improve the ability of networks to learn from data of this sort by representing the data with lower dimensional vectors.\n",
    "\n",
    "Word embeddings in particular are interesting because the networks are able to learn semantic relationships between words. For example, the embeddings will know that the male equivalent of a queen is a king.\n",
    "\n",
    "<img src=\"images/linear-relationships.png\" alt=\"Drawing\" style=\"width: \n",
    "500px;\"/>\n",
    "\n",
    "These word embeddings are learned using a model called **[Word2vec](https://en.wikipedia.org/wiki/Word2vec)**. In this lesson, we'll implement Word2vec.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec/Embeddings Details on what Word2Vec is & Lab on how to build solution** at this location : https://github.com/anshoomehra/udacity-deep-learning/tree/master/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using RNNs\n",
    "\n",
    "Details wrt how to use RNN for Sentiment Analysis & implementation details ... \n",
    "https://github.com/anshoomehra/udacity-deep-learning/tree/master/sentiment-rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
