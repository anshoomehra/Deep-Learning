{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs).\n",
    "\n",
    "The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous(also known as **temporal**) inputs when generating the current output. In other words, our systems did not have any **memory** elements. RNNs address this very basic and important issue by using **memory** (i.e. past inputs to the network) when producing the current output.\n",
    "\n",
    "** RECURRENT ** - Occuring often or reapeatedly\n",
    "\n",
    "So what do we call these networks RNN, it is because we perform the same task for each element in the input sequence.\n",
    "\n",
    "RNN are similar to feed-forward networks, let have a quick refresher of feed-forward network..\n",
    "\n",
    "<img src=\"images/rnn1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## Before we go deep, let's study bit of history of how RNN evolved ..\n",
    "\n",
    "After the first wave of **FFNN (Feed Forward Neural Networks) in 1980s**, it was clear that they have limilation as they are unable to handle temporal dependencies, which as we mentioned above that dependecies which change over time.\n",
    "\n",
    "<img src=\"images/rnn2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** Modeling temporal data is critical in most real-world applications, since natural signals like speech and video have time variant properties & are characterized by having dependencies across time. **\n",
    "\n",
    "Btw, Biologocal Neural Networks have recurring connections, hence, applying to recurrence to artificial feedfoward neural networks made natural sense.\n",
    "\n",
    "**First Attempt** to add memory to Neural Networks were the **Time Delay Neural Networks, TDNNs in short **. In TDNNS, inputs from past timesteps were introduced to the Neural Input, changing the actual external inputs. This had the advantage to have network look beyond current time step, but also introduced the clear disadvantage, since the temporal dependecies were limited to the window of time chosen.\n",
    "\n",
    "<img src=\"images/rnn3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** Simple RNN & ELMON NETWORK (1990)** were next to follow..  \n",
    "\n",
    "<img src=\"images/rnn4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "It was recognised that all these networks in early 90s were suffered from what we call, ** Vanishing Gradient problem **, in which contributions of information decayed geometrically over time.. Hence,\n",
    "capturing relationships that spanned more than eight or ten steps back was practically impossible.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In mid 90s ** Long Short-Term Memory (LSTM) ** Cells were invented to tackle vanishing gradient, susequently fundamental issue of adding memory to these networks.\n",
    "\n",
    "<img src=\"images/rnn6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The novelty in LSTMs was the idea that some , what we called state variables, can be kept fixed using gates & re-introduced or not at an appropriate time in future. In this way, arbitraty time intervals can be represented, and temporal dependencies can be captured.\n",
    "\n",
    "Variations in LSTM such as, ** Gated Recurrent Networks (GRUs) ** further refined this there and nowadays represent another mainstream approach to realizing RNNs.\n",
    "\n",
    "\n",
    "*************\n",
    "\n",
    "** Optional Read: **\n",
    "\n",
    "1. What does Vanishing Gradient mean??\n",
    "\n",
    "While training our network we use backpropagation. In the backpropagation process we adjust our weight matrices with the use of a gradient. In the process, gradients are calculated by continuous multiplications of derivatives. The value of these derivatives may be so small, that these continuous multiplications may cause the gradient to practically \"vanish\"\n",
    "\n",
    "detailed read..\n",
    "https://en.wikipedia.org/wiki/Vanishing_gradient_problem\n",
    "\n",
    "Geometric Series and how its values may exponentaially decrease..\n",
    "https://socratic.org/algebra/exponents-and-exponential-functions/geometric-sequences-and-exponential-functions\n",
    "\n",
    "2. If you are still curious, for more information on the important milestones mentioned here, please take a peek at the following links:\n",
    "\n",
    "**TDNN** https://en.wikipedia.org/wiki/Time_delay_neural_network\n",
    "\n",
    "** Original ELMAN Network Publication of 1990 ** http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract . To simplify this, this link can be studied as well https://en.wikipedia.org/wiki/Recurrent_neural_network#Elman_networks_and_Jordan_networks\n",
    "\n",
    "**LSTM** http://www.bioinf.jku.at/publications/older/2604.pdf -- In this LSTM link you will find the original paper written by Sepp Hochreiter and JÃ¼rgen Schmidhuber\n",
    "\n",
    "**GRUs** https://deeplearning4j.org/lstm.html  (begineer Guide to RNN & LSTMs )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are key applications of RNN?\n",
    "\n",
    "1. Speech Recognition : \n",
    ">Google Assistant <br>\n",
    ">Amazon Alexa<br>\n",
    ">Apple Siri<br>\n",
    ">Nuances Dragon<br>\n",
    "\n",
    "2. Time Series Predictions: \n",
    "> Traffic Patterns : Waze<br>\n",
    "> Movie Selection : Netflix <br>\n",
    "> Stock Movement : Hedge Funds<br>\n",
    "\n",
    "3. NLP\n",
    "> Machine Translation : Google, Salesforce<br>\n",
    "> Q&A : Google Analytics<br>\n",
    "> Chatbots : Slack, Google, Baidu<br>\n",
    "\n",
    "4. Gesture Recognition\n",
    "\n",
    "5. Are you into gaming and bots?\n",
    "> DotA 2 bot by Open AI : https://blog.openai.com/dota-2/\n",
    "\n",
    "6. How about automatically adding sounds to silent movies? https://www.youtube.com/watch?time_continue=1&v=0FW99AQmMc8\n",
    "\n",
    "7. Here is a cool tool for automatic handwriting generation. http://www.cs.toronto.edu/~graves/handwriting.cgi?text=My+name+is+Luka&style=&bias=0.15&samples=3\n",
    "\n",
    "8. Amazon's voice to text using high quality speech recognition, Amazon Lex. https://aws.amazon.com/lex/faqs/\n",
    "\n",
    "9. Facebook uses RNN and LSTM technologies for building language models. https://code.facebook.com/posts/1827693967466780/building-an-efficient-neural-language-model-over-a-billion-words/\n",
    "\n",
    "10. Netflix also uses RNN models - here is an interesting read. https://arxiv.org/pdf/1511.06939.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Requisite\n",
    "\n",
    "I will skip over details here, see NN in Nut Shell Jupyter, below is quick summary of what one should know before hand .. \n",
    "\n",
    "** Feed-Forward NN (FFNN) **\n",
    "\n",
    "<img src=\"images/rnn7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Quick Refresher on **Activation Functions**: https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions\n",
    "\n",
    "**tanh** = -1 to 1 <br>\n",
    "**sigmoid** = 0 to 1 <br>\n",
    "**ReLU** = Work liek switch, turn -ve values to zero & +ve values remains as they are.<br>\n",
    "(Each have they advantages and dis-advantages)\n",
    "\n",
    "<img src=\"images/rnn9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Error Functions:** The two error functions that are most commonly used are the **Mean Squared Error (MSE)** (usually used in regression problems & also called **Log Function**) and the **Cross Entropy** (usually used in classification problems & also called **Log Loss Function**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs and RNNs can be combined\n",
    "\n",
    "CNN can be used to extract features & RNN can be used for memory injection, popular application gesture recognition.\n",
    "\n",
    "<img src=\"images/rnn8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Backpropagation **\n",
    "\n",
    "**Derivatives: ** http://www.columbia.edu/itc/sipa/math/calc_rules_multivar.html\n",
    "\n",
    "**Common Derivates Cheat Sheet : **http://tutorial.math.lamar.edu/pdf/Common_Derivatives_Integrals.pdf\n",
    "\n",
    "** Tuning Learning Rate in Gradient Descent ** : http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/#loss\n",
    "\n",
    "\n",
    "** Overfitting ** \n",
    "> Mitigations\n",
    "> 1. Stopping Early\n",
    "> 2. Regularization (Dropout)\n",
    "\n",
    "**Mini-Batching** Updating the weights every N Steps\n",
    "> Reduction of the complexity if the training process\n",
    "> Noise Reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have refreshed all key subjects before and are ready to start RNNs now .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we know thus far is RNNs can maintain previous inputs by maintaining internal memory elements, also known as **States**\n",
    "\n",
    "<img src=\"images/rnn10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Many applications have temporal dependencies, which means Output they just do not depend on Current Input but also takes in to account Past Inputs. For cases like these we need to use RNNs\n",
    "\n",
    "<img src=\"images/rnn11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "A good and simple example for RNN could be 'Predicting Next Word in the Sentence', which typically requires looking at the last few words rather than current one\n",
    "\n",
    "<img src=\"images/rnn12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "And we already discussed some other relevant areas of study where RNN is apt to be used .. and frankly use cases of RNN are just popping up everywhere and hard to keep up with ..\n",
    "\n",
    "<img src=\"images/rnn13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "RNN are very similar to FFNN, they have very similar input and output patterns like, many-to-many, many-to-one, one-to-many.\n",
    "\n",
    "However RNNs have 2 fundamental differences from FFNNs, which are:\n",
    "\n",
    "**1st Difference: Seuqences as Inputs** - Instead of training network with single input, single output at each time step, we train with sequences since previous input matters.\n",
    "\n",
    "**2nd Difference: Memeory Elements** - Stems from the memory elements RNNs host, current inputs, as well as activations of neurons serve as inputs to the next time step.\n",
    "\n",
    "<img src=\"images/rnn14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In FFNN we saw flow from input to the outout w/o any feedback ..\n",
    "\n",
    "<img src=\"images/rnn15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Now, the FF scheme changes & includes feedback or memory elements. We will define memory as the output of the hidden layer, which will serve as an additinal input to the network at the following training step.\n",
    "\n",
    "<img src=\"images/rnn16.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We will no longer use H as the output of the hidden layer, but S as state referring to system with memory.\n",
    "\n",
    "<img src=\"images/rnn17.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The basic scheme of RNN or on other words basic three layer neural network with feedback that serve as memory inputs is called  **Simple RNN or Elman Network**.\n",
    "Not that above pictire depicts nth outputs which is feasible, but we will use 2 outputs to understand foundation better and prior looking at complex networks ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So let's dwell bit deep on how RNN works .. \n",
    "\n",
    "As we discussed earlier that RNNs have unique way of injecting States ( aka memort units) in to the network. Let's see how can this be implemented.\n",
    "\n",
    "As we've see, in FFNN the output at any time t, is a function of the current input and the weights. This can be easily expressed using the following equation:\n",
    "\n",
    "y_hat = F($x_{t}$,W)\n",
    "\n",
    "In RNNs, our output at time t, depends not only on the current input and the weight, but also on previous inputs. In this case the output at time t will be defined as:\n",
    "\n",
    "y_hat = F ($x_{t}$, $x_{t-1}$, $x_{t-2}$, ..., $x_{t-t0}$,W)\n",
    "\n",
    "This is called RNN ** Folded Model **.\n",
    "\n",
    "<img src=\"images/rnn18.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "The model can also be \"unfolded in time\". The unfolded model is usually what we use when working with RNNs.\n",
    "\n",
    "<img src=\"images/rnn19.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In both the folded and unfolded models shown above the following notation is used:\n",
    "\n",
    "x represents the input vector, <br>\n",
    "y represents the output vector and <br>\n",
    "s denotes the state vector.\n",
    "\n",
    "$W_{x}$ is the weight matrix connecting the inputs to the state layer.\n",
    "\n",
    "$W_{y}$ is the weight matrix connecting the state layer to the output layer.\n",
    "\n",
    "$W_{s}$ represents the weight matrix connecting the state from the previous timestep to the state in the current timestep.\n",
    "\n",
    "In FFNNs the hidden layer depended only on the current inputs and weights, as well as on an activation function \\PhiÎ¦ in the following way:\n",
    "\n",
    "h = Î¦(x W)\n",
    "\n",
    "In RNNs the state layer depended on the current inputs, their corresponding weights, the activation function and also on the previous state:\n",
    "\n",
    "<img src=\"images/rnn20.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "** RNNs can be stacked like Lego's and can derive more interesting outcomes, as in practice, it does not matter where inputs comes from and what outputs we are headed too ..**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Through Time (BPTT)\n",
    "\n",
    "We are now ready to understand how to train the RNN.\n",
    "\n",
    "When we train RNNs we also use backpropagation, but with a conceptual change. The process is similar to that in the FFNN, with the exception that we need to consider previous time steps, as the system has memory. This process is called ** Backpropagation Through Time (BPTT) **\n",
    "\n",
    "<img src=\"images/rnn21.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now unfold the model. You will see that unfolding the model in time is very helpful in visualizing the number of steps (translated into multiplication) needed in the Backpropagation Through Time process. These multiplications stem from the chain rule and are easily visualized using this model.\n",
    "\n",
    "\n",
    "<img src=\"images/rnn22.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn23.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn24.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step! Adjusting $W{_x}$, the weight matrix connecting the state to the output.\n",
    "\n",
    "<img src=\"images/rnn25.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn26.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn27.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn28.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/rnn29.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen Foward Pass & BPTT - we are ready to train, as reminder, we can follow **Mini Batching** concept here, i.e. we compute gradient at every step, but choose to update weights once every N Steps as shown below ..\n",
    "\n",
    "<img src=\"images/rnn30.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### What happens if we have many time steps, can we simply accumulate the gradient contributions from each of these steps ?? Answer is NO\n",
    "\n",
    "<img src=\"images/rnn31.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Studies have shown that fir upto a small number of time steps say 8 or 10, we can effectively used this method. Gimg beyond that we Gradient wukk become to osmall which is known as ** Vanishing Gradient Problem **. IN other words, temporal dependencies over 8 or 9 time steps will discarded by the network. \n",
    "\n",
    "### So how to solve Vanishing Gradient Problem ??\n",
    "\n",
    "LSTM - Long Short-Term memory Cells were invented specifically to address this problem and we will study this in detail ahead ..\n",
    "\n",
    "<img src=\"images/rnn32.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "### Exploding Gradients is another issue we should be aware ..\n",
    "\n",
    "In this gradient grows uncontrollably, luckily a simple scheme called **Gradient Clipping** resolves this issue.\n",
    "\n",
    "<img src=\"images/rnn33.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "Basically, we check if Gradient cross certain threshhold, if it does, we normalize (penalize) excessively large gradients crossing thresholds, and this helps solve Exploding Gradient problem.\n",
    "\n",
    "<img src=\"images/rnn34.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "In depth article on Gradient Clipping: https://arxiv.org/abs/1211.5063"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs\n",
    "\n",
    "**Long Short-Term Memory Cells, (LSTM)** give a solution to the vanishing gradient problem, by helping us apply networks that have temporal dependencies. They were proposed in 1997 by Sepp Hochreiter and JÃ¼rgen Schmidhuber\n",
    "\n",
    "If we take a close look at the RNN neuron, we can see that we have simple linear combinations (with or without the use of an activation function). We can also see that we have a single addition.\n",
    "\n",
    "Zooming in on the neuron, we can graphically see this in the following configuration:\n",
    "\n",
    "<img src=\"images/rnn35.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The **LSTM** cell is a bit more complicated. If we zoom in on the cell, we can see that the mathematical configuration is the following:\n",
    "\n",
    "<img src=\"images/rnn36.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The LSTM cell allows a recurrent system to learn over many time steps without the fear of losing information due to the vanishing gradient problem. It is fully differentiable, therefore gives us the option of easily using backpropagation when updating the weights.\n",
    "\n",
    "LSTMs have 4 different functions whcih are computed in 3 steps as listed below .. these help us retain \n",
    "\n",
    "<img src=\"images/rnn37.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference Study Material for RNNs:\n",
    "\n",
    "Chris Olah's Post:\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Edwin Chen's Post:\n",
    "http://blog.echen.me/2017/05/30/exploring-lstms/\n",
    "\n",
    "Andrej Karpathy's Lecture:\n",
    "https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Study LSTM with some examples...\n",
    "\n",
    "**Step 1** So we have NN below which receives input as image and predicts this as Dog\n",
    "\n",
    "<img src=\"images/rnn38.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Step 2** How do we know for sure that it is Dog and not Wolf ?\n",
    "\n",
    "This is where RNN comes in picture, if we have **memory** like show in example below, that we have been seeing forest animals, our output would most likely be inclined towards Wolf instead of Dog isn't it ??\n",
    "\n",
    "<img src=\"images/rnn39.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "** Step 3 ** What if the memory is not immediate (Short Term) and of recent past like this (i.e. Long Term)?? In case of RNNs, because of Vanishing Gradient, we will not be able to take advantage of that, instead immediate memory States (Short Term) will influence our decision and predict the outcome most likely as Dog instead .. \n",
    "\n",
    "<img src=\"images/rnn40.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "This is where **LSTM** comes in as it can keep track of both Short Term and Long Term memories.\n",
    "\n",
    "Quick summary of approach and differences between RNN and LSTMs\n",
    "\n",
    "**RNN** Memory + Current Input combined derive two outcomes,first prediction, second, new memory whcih act as input to next State.\n",
    "\n",
    "<img src=\"images/rnn41.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**LSTM** LSTMs are very similar, with difference that it keep track of two memories ** Long Term and Short Term **, so the present state depends on 3 inputs instead, i.e. Long Term Memory, Short Term Memory and Current Input all these 3 inputs gets merged to derive 3 outputs, prediction, and new Short Term + Long Term Memory.\n",
    "\n",
    "<img src=\"images/rnn42.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of RNN vs LSTM\n",
    "\n",
    "We have studied RNN at depth and hence will not go over any details here ..\n",
    "<img src=\"images/rnn43.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "LSTMs architecture looks very complicated with many functions inside, but it is not as complex as it looks, let's look at it in detail ahead ..\n",
    "\n",
    "<img src=\"images/rnn44.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gates of LSTM \n",
    "\n",
    "There are four main gates .. \n",
    "1. Learn Gate \n",
    "2. Forget Gate \n",
    "3. Remember Gate\n",
    "4. Use Gate\n",
    "\n",
    "Let's study how these gates work .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Learn Gate \n",
    "Let's look at memories we have on the given example ...\n",
    "\n",
    "<img src=\"images/rnn45.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Learn Gate takes these inputs and combine them .. \n",
    "\n",
    "<img src=\"images/rnn46.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Actually it does more than combining them ..\n",
    "\n",
    "Do first it combine inputs .. \n",
    "\n",
    "<img src=\"images/rnn47.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "And then ignores the non-relevant part, in this case, Tree is non-relevant, so it ignores the tree .. \n",
    "\n",
    "<img src=\"images/rnn48.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "And subseuqently the final output as ..\n",
    "\n",
    "<img src=\"images/rnn49.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "** So how does this work Mathematically **\n",
    "\n",
    "**Combine** As we have seen before, It takes Input, Short Term Memory combone them using activation (in this example tanh) ..\n",
    "\n",
    "<img src=\"images/rnn50.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Ignore**\n",
    "\n",
    "We forget **Multiplying** by **Ignore Factor $i{_t}$ **, it is actually a vector byt get multiplied elementwise. So how do we compute $i{_t}$ **? So we take previous information of out short term memory & the event, creates a small NN, passing them through small linear function with a **New matrix $W{_i}$ & Bias **, squish them with sigmoid function to keep it between zero and one .. and that is it, this is how learn gate works .. \n",
    "\n",
    "<img src=\"images/rnn51.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Forget Gate \n",
    "\n",
    "Forget takes the input as Long Term Memory and try to compute what to forget ..\n",
    "\n",
    "<img src=\"images/rnn52.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So in this example, we have memories of Nature and Science and Forget Gate decide to Keep Nature and Forget Science ..\n",
    "\n",
    "<img src=\"images/rnn53.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So how does it work mathematically .. ??\n",
    "\n",
    "It takes **Long Term Meory State** and **Multiplies** the same with **Forget Factor**, and we compute **Forget Factor** as before, by creating small NN, which takes **Short Term Memory** and **Input**, linearly combine them to compute **Forget Factor**.\n",
    "\n",
    "<img src=\"images/rnn54.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Remember Gate \n",
    "\n",
    "It combines LTM coming out of Forget Gate && STM coming out of Learn Gate and simply combine them.\n",
    "\n",
    "<img src=\"images/rnn55.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Mathematically it is simple **Addition Function** \n",
    "\n",
    "<img src=\"images/rnn56.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Gate (aka Output Gate)\n",
    "\n",
    "Use Gate uses LTM coming out of Forget Gate && STM coming out of Learn Gate and simply combine keeping what is relevant to produce output as ** New Short Term Memory **.. In this case, it takes bear from LTM and Squirrel from STM along with Input as new STM ..\n",
    "\n",
    "** Note Image below has Error, Output is shown as New LTM instead of STM **\n",
    "\n",
    "<img src=\"images/rnn57.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "Mathemtaically, it applies tanh on  LTM coming out of Forget Gate && Sigmoid on STM coming out of Learn Gate, further Multiply them to compute New STM\n",
    "\n",
    "<img src=\"images/rnn58.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together ...\n",
    "\n",
    "<img src=\"images/rnn59.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "<img src=\"images/rnn60.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "So there may be curiosity that this is all abritary, some places we use tanh, some sigmoid, multiplication, addition etc. IN fact it is arbitraty however has been proven to work, one can experiment, this area is very much under development + research.\n",
    "\n",
    "Reference for Detailed Study : https://deeplearning4j.org/lstm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Architectures that work ..\n",
    "\n",
    "**GRU - Gated Recurrent Unit** It combines Forget & Learn gate into Update Gate & runs this through Combine Gate. It also working with only one Working Memory insteas of LTM and STM - but in practce this works very well.\n",
    "\n",
    "<img src=\"images/rnn61.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "**Peephole Connections:** ie using LTM as well to calculate forget factor ..\n",
    "<img src=\"images/rnn62.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "<img src=\"images/rnn63.png\" alt=\"Drawing\" style=\"width: \n",
    "700px;\"/>\n",
    "\n",
    "\n",
    "Detailed References to Study:\n",
    "\n",
    "Michael Guerzhoy's post\n",
    "http://www.cs.toronto.edu/~guerzhoy/321/lec/W09/rnn_gated.pdf\n",
    "\n",
    "Steve Carell\n",
    "http://despicableme.wikia.com/wiki/Felonius_Gru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-wise RNN\n",
    "\n",
    "This network will learn about some text one character at a time & then generate new text one character at a time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
