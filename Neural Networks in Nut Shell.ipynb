{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent to use this guide as refresher for course material studied for  Neural Networks from Udacity.\n",
    "\n",
    "This guide does not go in depth explaining fundamentals, one should go through proper course material prior using this guide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTRONS : Wx + b = 0\n",
    "\n",
    "W = Weights\n",
    "\n",
    "X = Inputs\n",
    "\n",
    "b = Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## AND Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                  -2.0                    0          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                  -1.0                    0          Yes\n",
      "      1          1                   0.0                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = 1.0\n",
    "weight2 = 1.0\n",
    "bias = -2.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOT Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Linear Combination    Activation Output   Is Correct\n",
      "      0          0                   1.0                    1          Yes\n",
      "      0          1                  -1.0                    0          Yes\n",
      "      1          0                   0.0                    1          Yes\n",
      "      1          1                  -2.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO: Set weight1, weight2, and bias\n",
    "weight1 = -1.0\n",
    "weight2 = -2.0\n",
    "bias = 1.0\n",
    "\n",
    "\n",
    "# DON'T CHANGE ANYTHING BELOW\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(linear_combination >= 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV contains 100 rows & 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X[0]</th>\n",
       "      <th>X[1]</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.78051</td>\n",
       "      <td>-0.063669</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.28774</td>\n",
       "      <td>0.291390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.40714</td>\n",
       "      <td>0.178780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.50922</td>\n",
       "      <td>0.352560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X[0]      X[1]  y\n",
       "0  0.78051 -0.063669  1\n",
       "1  0.28774  0.291390  1\n",
       "2  0.40714  0.178780  1\n",
       "3  0.29230  0.421700  1\n",
       "4  0.50922  0.352560  1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv('data.csv', names=['X[0]', 'X[1]', 'y'])\n",
    "\n",
    "print (\"CSV contains {} rows & {} columns\".format(data_df.shape[0], data_df.shape[1]))\n",
    "\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data File to separate X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "      X[0]      X[1]\n",
      "0  0.78051 -0.063669\n",
      "1  0.28774  0.291390\n",
      "2  0.40714  0.178780\n",
      "3  0.29230  0.421700\n",
      "4  0.50922  0.352560\n"
     ]
    }
   ],
   "source": [
    "X = data_df.iloc[:,0:2].copy()\n",
    "print (X.shape)\n",
    "print (X.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_df.iloc[:,2].copy()\n",
    "y.shape\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert dataframe to matrix to strip headers & also use easy slicing method as used by base functions above\n",
    "\n",
    "X = X.as_matrix()\n",
    "y = y.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Base Functions for Perceptron\n",
    "\n",
    "Note: **Step Function** is additional step which takes score from perceptron equation Wx+b=0 and convert the same into 0 or 1 as final output. This would help achieve sort of Boolean nature resultset as we have in dataset predicting whether perdiction (y_hat) is in line with y (actual label)\n",
    "\n",
    "**Perceptron Trick**: So how do we draw the line in a way that it does best job filtering data or drawing line to separate the data for perdiction purposes.\n",
    "\n",
    "So as example if line equation is 3x‚ÇÅ + 4x‚ÇÇ - 10 = 0  (Wx+b=0)\n",
    "\n",
    "And if our goal is to go closer to point (example 4,5) on Positive Side(i.e. correct lables), what we do is minus Weights from these points & 1 from Bias Unit .. i.e. 3-4x‚ÇÅ + 4-5x‚ÇÇ + (-10-1) = 0 which simplifies to\n",
    "-x‚ÇÅ - x‚ÇÇ -11 = 0  and this trick will possibly not just go closer to line but even cross the line.\n",
    "\n",
    "To go at slower progression, we introduce something called as ** Learning Rate **, it is nothing but factor of point(4,5) - say if learning rate is 0.1 then pur equation would be\n",
    "\n",
    "3-.4x‚ÇÅ + 4-.5x‚ÇÇ + (-10-.1) = 0 which simplifies to\n",
    "\n",
    "2.6x‚ÇÅ + 3.5x‚ÇÇ -10.1 = 0\n",
    "\n",
    "And, we do this n number if times until our **Error Rate** reduce, which introduces two new terms,\n",
    "\n",
    "**Error Rate** is the distance of line from the point, father it is higher the error, hence goal is to reduce the error.\n",
    "\n",
    "**Epochs** is the  number iterations we run to make line come closer to points or in other words reduce the error to 0 or minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Setting the random seed, feel free to change it and see different solutions.\n",
    "np.random.seed(42)\n",
    "\n",
    "def stepFunction(t):\n",
    "    if t >= 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def prediction(X, W, b):\n",
    "    #print (\"Anshoo\", X[i].shape, W.shape)\n",
    "    return stepFunction((np.matmul(X,W)+b)[0])\n",
    "\n",
    "# TODO: Fill in the code below to implement the perceptron trick.\n",
    "# The function should receive as inputs the data X, the labels y,\n",
    "# the weights W (as an array), and the bias b,\n",
    "# update the weights and bias W, b, according to the perceptron algorithm,\n",
    "# and return W and b.\n",
    "def perceptronStep(X, y, W, b, learn_rate = 0.01):\n",
    "    for i in range(len(X)):\n",
    "        y_hat = prediction(X[i],W,b)\n",
    "        if y[i]-y_hat == 1:\n",
    "            W[0] += X[i][0]*learn_rate\n",
    "            W[1] += X[i][1]*learn_rate\n",
    "            b += learn_rate\n",
    "        elif y[i]-y_hat == -1:\n",
    "            W[0] -= X[i][0]*learn_rate\n",
    "            W[1] -= X[i][1]*learn_rate\n",
    "            b -= learn_rate\n",
    "    return W, b\n",
    "    \n",
    "# This function runs the perceptron algorithm repeatedly on the dataset,\n",
    "# and returns a few of the boundary lines obtained in the iterations,\n",
    "# for plotting purposes.\n",
    "# Feel free to play with the learning rate and the num_epochs,\n",
    "# and see your results plotted below.\n",
    "def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):\n",
    "    x_min, x_max = min(X.T[0]), max(X.T[0])\n",
    "    y_min, y_max = min(X.T[1]), max(X.T[1])\n",
    "    \n",
    "    #print(\" Min and Max for x & y..\", x_min, x_max,y_min, y_max )\n",
    "    \n",
    "    #W = np.array(np.random.rand(2,1))\n",
    "    #TRY BOTH WEIGHTS, RANDOM WEIGHTS DO NOT SCALE WELL,\n",
    "    #IF WE NORMALIZE THE WEIGHTS, WE GET BETTER RESULTS\n",
    "    W = np.random.normal(scale=1/1**.5, size=(2,1))\n",
    "    \n",
    "    b = np.random.rand(1)[0] + x_max\n",
    "    # These are the solution lines that get plotted below.\n",
    "    boundary_lines = []\n",
    "    for i in range(num_epochs):\n",
    "        # In each epoch, we apply the perceptron step.\n",
    "        W, b = perceptronStep(X, y, W, b, learn_rate)\n",
    "        boundary_lines.append((-W[0]/W[1], -b/W[1]))\n",
    "    return boundary_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to Run the Perceptron Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boundary_lines_to_plot = trainPerceptronAlgorithm(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'zip' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-07fe462d182d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mb_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplot_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-07fe462d182d>\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(m, b, color)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0ml_eq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_eq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'zip' and 'float'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b874dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_points(X, y):\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
    "\n",
    "# M IS SLOPE, B IS INTERCEPT FOR LINE EQUATION\n",
    "def display(m, b, color='g--'):\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    x = np.arange(-10, 10, .80)\n",
    "    l_eq = (m_*x+b_)[0]\n",
    "    plt.plot(x, l_eq, color)\n",
    "\n",
    "    \n",
    "m,b = zip(*boundary_lines_to_plot)\n",
    "m_ = zip(*m)\n",
    "b_ = zip(*b)\n",
    "\n",
    "display(m_,b_)\n",
    "plot_points(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss Error Function\n",
    "\n",
    "So we talked about error rate briefly above. However, we did not apply error rate in our calculation yet, which we wil do now, however, before we do so, the error rate must be **continous** in nature, i.e every iteration should tell us whether we are getting closer to our goal or heading father .. \n",
    "\n",
    "This is good segue to introduce **Log Loss Error Method**. This method introduce penalty to every point or input data, points already in right classified zones have least penalty and points classified in wrong zones have highest penalties, so at every epoch, we access total error, with goal to minimize or completely eliminate the error. \n",
    "\n",
    "Since we are able to build error function which can continuouly calculate the error, we can use **Gradient Descent** to solve our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Predictions with Sigmoid Function\n",
    "\n",
    "Step Function are discrete in nature, which tells whether prediction matches actual label or not, however, since we are able to calculate error which is continuous in nature, we will have to calulate perdictions which are also continuous in ouput i.e. knowing how close we are or how off we are to the actual, Step Function does not provide that information, this is where **Sigmoid** function come sin handy, this function tells us probabiliy of how far or close we are to our goal.\n",
    "\n",
    "sigmoid(x) = 1/(1+e-x)\n",
    "\n",
    "So if we were to compare to Step Function of Values 0 and 1, Sigmoid will give probability close to 0% for negatives values and close to 100% for positive values && 50% for (0,0), i.e. mid point.\n",
    "\n",
    "Just to put it all together, we will repalce Step Function with Sigmoid to achieve continuous output.\n",
    "\n",
    "y_hat_discrete = StepFunction(Wx+b)\n",
    "\n",
    "y_hat_continuous = SigmoidFunction(Wx+b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Solution using Softmax Function\n",
    "\n",
    "Before we go see how to write Softmax, let's see why we have to use this. So we studied about continuous perdictions using Sigmoid Function but that solution work well if we have one class as an output and subsequent probalities, what if we have more than one class as an output. Well we do have scores which are higher for respectives classes however to derive accurate probablities we will have to normalize scores, example : if we are predicting  breeds of Dog, with output as Golden Retreiver, German Shepherd, Autralian Shepherd and our equation produce scores like:\n",
    "\n",
    "Golden Retreiver = 2\n",
    "\n",
    "German Shepherd = 1\n",
    "\n",
    "Autralian Shepherd = 0\n",
    "\n",
    "we can normalize as 2/(2+1+0), 1/(2+1+0), 0/(2+1+0)\n",
    "\n",
    "however there is one major issues, what if one of the scores is negative, which is plausible, this may turn divisor to zero, and in turn probability to infinity which does not make sense.\n",
    "\n",
    "Idea is good though, to solve this, one can make use of  exponential function, as the result of that is always positive, so if we apply this our normalization equation, we get something line this ..\n",
    "\n",
    "Softmax Function = e(Xi) / ( e(Xi) +..+ e(Xn) )\n",
    "\n",
    "Applying this to our example equation:\n",
    "\n",
    "SF_Probablities = e(2)/(e(2)+e(1)+e(0)), e(1)/(e(2)+e(1)+e(0)), e(0)/(e(2)+e(1)+e(0))\n",
    "SF_Probablities = 67%, 24%, 9%\n",
    "\n",
    "for respective breeds of dogs. As always these must add upto 100%.\n",
    "\n",
    "So let's dive into coding of Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    sumExpL = sum(expL)\n",
    "    result = []\n",
    "    for i in expL:\n",
    "        result.append(i*1.0/sumExpL)\n",
    "    return result\n",
    "\n",
    "L = (1,2,3)\n",
    "\n",
    "sm_values_ret = softmax(L)\n",
    "\n",
    "print sm_values_ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "\n",
    "So it may be obvious that all our outputs are numerical, but it is not always that we have numerical inputs, we can convert them though.\n",
    "\n",
    "It is easy in case of one class, we can define true as 1 and false as 0, but what if we have more classes, the approach which helps is called One-Hot Encoding.\n",
    "\n",
    "What we do here is create matrix for clases, and mark when true as example below:\n",
    "\n",
    "        Input Class     Golden Retreiver?     German Shepherd?    Autralian Shepherd?\n",
    "    Golden Retreiver          1                     0                   0\n",
    "    German Shepherd           0                     1                   0\n",
    "    Autralian Shepherd        0                     0                   1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "\n",
    "We pick the model which gove labels the highest probability, thus by maximizing the probability we pick the best possible model.\n",
    "\n",
    "wrt computing this, we take the probabilities of each input of what it actually is, i.e. probability of RED being RED and BLUE being BLUE, multiply these probabilities across models, and model giving the highest P(all) is our goal and is also the best model.\n",
    "\n",
    "So thus far we can summarize that **Probabilities are Important** and that **Better Model gives better overall Probability i.e. P(all)**, so how do we maximize probability, we also studied **Error Function**, and that minimizing error function gives better model, so is there a connection??, Let's see ..\n",
    "\n",
    "However, before we go further, we are highly dependent on Product(Multiplication) to find maximized probablities so far, and products could be nasty in terms of computation, specifically when we compute over thousands of samples, and these tiny numbers which all sum upto 1 could lead to errors. We know that addition is better than multiplication in this case, and the best way we can covert this using addition is by using **log**, we know that \n",
    "\n",
    "**log(ab) = log(a) + log(b)** and this is precisely we need to ease our job .. \n",
    "\n",
    "One quick thing though, natural log(1) = 0, which means log values for our probabilities which are under 1 will be in -ve, to fix this, we will instead do - log(probability), this will take care negative values.\n",
    "\n",
    "Our revised equation would be:<br>\n",
    "** For individual point : -log(probablity)** <br>\n",
    "** Overall it can demostrated as: -log(a) + -log(b) .. + -log(n)**\n",
    "\n",
    "### Cross Entropy\n",
    "The above approach of taking ** -log(probability) ** is also referred as Cross Entropy. Key thing to note here is that, **Higher the Probability results into Lower Cross Entropy**, which makes sense since -log(p) will result in low number for higher values. This is also true for all individual points(inputs), which means if we look at log(p) for each point, ** higher values means higher ERROR rate** * **lower value means lower ERROR rate**. [ Just as reminder, we were caluclating distance of point from line earlier to derive error rate ]\n",
    "\n",
    "Having said this, our goal dramatically has changed from ** Maximizing Probablity to Minimizing Cross Entropy **\n",
    "\n",
    "So we're getting somewhere, there's definitely a connection between probabilities and error functions, and it's called Cross-Entropy. This concept is tremendously popular in many fields, including Machine Learning. Let's dive more into the formula, and actually code it!\n",
    "\n",
    "Formula for CE for Two Classes is simple\n",
    "\n",
    "** CE = - ‚àë·µ¢( Y·µ¢*ln(P·µ¢) + (1-Y·µ¢)*ln(1-P·µ¢) **\n",
    "\n",
    "For multiple classes, this can be revised as..\n",
    "\n",
    "** CE = - ‚àë·µ¢ ‚àëùöì Y·µ¢ùöì*ln(P·µ¢ùöì) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float_(Y)\n",
    "    P = np.float_(P)\n",
    "    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n",
    "\n",
    "# Simplified version below ..\n",
    "#def cross_entropy(Y, P):\n",
    "#    CE = 0\n",
    "#    \n",
    "#    for i in range(len(P)):\n",
    "#        CE += Y[i]*np.log(P[i]) + (1-Y[i]) * np.log(1-P[i])\n",
    "#        \n",
    "#    return -CE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with above we can revise our **'Error Funtion'** as below:\n",
    "\n",
    "Taking example of differnetiating red dots with blue dots ..\n",
    "\n",
    "If y = 1\n",
    "P(Blue) = y_hat\n",
    "Error = -ln(y_hat)\n",
    "\n",
    "\n",
    "If y = 0\n",
    "P(Red) = 1-P(Blue) == 1-y_hat\n",
    "Error = -ln(1- y_hat)\n",
    "\n",
    "Overall Error:\n",
    "\n",
    "Error = -(1-y)(ln(1-y_hat)) - y.ln(y_hat)\n",
    "\n",
    "Error Function = -1/m ‚àë·µ¢‚Çå‚ÇÅ‚Ä§‚Ä§m  (1-Y·µ¢)(ln(1-y·µ¢_hat)) + Y·µ¢*ln(y·µ¢_hat) \n",
    "\n",
    "Rewriting the same in terms if W,b\n",
    "\n",
    "** Error Function Binary Class **\n",
    "\n",
    "    E = -1/m ‚àë·µ¢‚Çå‚ÇÅ‚Ä§‚Ä§m  (1-Y·µ¢)(ln(1-y·µ¢_hat)) + Y·µ¢*ln(y·µ¢_hat)\n",
    "    E(W,b) = -1/m ‚àë·µ¢‚Çå‚ÇÅ‚Ä§‚Ä§m  (1-Y·µ¢)(ln(1-œÉ(Wx‚Å±+b)) + Y·µ¢*ln(œÉ(Wx‚Å±+b))\n",
    "\n",
    "\n",
    "** Error Function Multi-Class **\n",
    "\n",
    "    E = -1/m ‚àë·µ¢‚Çå‚ÇÅ‚Ä§‚Ä§m  ‚àëùöì‚Çå‚ÇÅ‚Ä§‚Ä§n Y·µ¢ùöì*ln(≈∑·µ¢ùöì)\n",
    "    E(W,b) = -1/m ‚àë·µ¢‚Çå‚ÇÅ‚Ä§‚Ä§m  ‚àëùöì‚Çå‚ÇÅ‚Ä§‚Ä§n Y·µ¢ùöì*ln(œÉ(Wx‚Å± ≤+b))\n",
    "    \n",
    "** These are same formula's as listed above in Cross Entropy Section, just elaborated for W,b and with one example .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression \n",
    "\n",
    "Logistic regression is named for the function used at the core of the method, the logistic function.\n",
    "\n",
    "Logistic regression uses an equation as the representation, very much like linear regression. Input values (X) are combined linearly using weights or coefficient values to predict an output value (y).\n",
    "\n",
    "A key difference logistic regression demostrate that the output value being modeled is a binary value (0 or 1) rather than a numeric value Linear Regression demonstrate\n",
    "\n",
    "Our goal is to reduce overall **Error (i.e Cross Entropy to be lower)**\n",
    "\n",
    "**Gradient Descent** is the process of minimizing a function by following the gradients of the cost function.\n",
    "\n",
    "We start with some random Weights and Bias<br>\n",
    "œÉ(Wx+b)\n",
    "\n",
    "and by applying **Gradient Descent** derive new weights and bais which optmize our line or in other words reduce error, we refer new weights as W prime and b prime..<br>\n",
    "œÉ(W'x+b')\n",
    "\n",
    "So how do we compute **Gradient Descent Step** mathemmatically, let's see it below ..\n",
    "\n",
    "Taking a step means, we will compute:<br>\n",
    "W' = W·µ¢ - Œ± ùî°E/ùî°W·µ¢<br>\n",
    "b' = b - Œ± ùî°E/ùî°b\n",
    "\n",
    "Œ± - Learning Rate <br>\n",
    "ùî° - Derivative\n",
    "\n",
    "So by using new Weights and Bias we can say that new Prediction is better than what we have with prior.<br>\n",
    "≈∑ = œÉ(W'x+b')\n",
    "\n",
    "With this let's get hand dirty by finding Derivatives of Error Function wrt W or b..\n",
    "\n",
    "Sigmoid Function has really nice derivatives ..<br>\n",
    "œÉ‚Ä≤(x)=œÉ(x)(1‚àíœÉ(x))\n",
    "\n",
    "Without explaing the complex math ..\n",
    "\n",
    "** ùî°E/ùî°Wùöì = -(y-≈∑)Xùöì <br>**\n",
    "** ùî°E/ùî°b = -(y-≈∑) **\n",
    "\n",
    "In other words ..\n",
    "‚àáE = ‚àí(y‚àí≈∑)(x1,....xn,1)\n",
    "\n",
    "If you think about it, this is fascinating. The gradient is actually a scalar times the coordinates of the point! And what is the scalar? Nothing less than a multiple of the difference between the label and the prediction. What significance does this have?\n",
    "\n",
    "    Closer the label to the prediction smaller the gradient\n",
    "    OR Father the label from the prediction, larger the gradient.\n",
    "    \n",
    "Let's revisit **Gradient Descent Step**\n",
    "\n",
    "Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:\n",
    "\n",
    "w‚Ä≤·µ¢ = w·µ¢ ‚àí Œ±[‚àí(y‚àí≈∑)x·µ¢]\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "** w‚Ä≤·µ¢ = w·µ¢ + Œ±(y‚àí≈∑)x·µ¢ **\n",
    "\n",
    "Similarly, it updates the bias in the following way:\n",
    "\n",
    "** b' = b - Œ±(y‚àí≈∑) **\n",
    "\n",
    "Note: Since we've taken the average of the errors, the term we are adding should be 1/m‚ãÖŒ± instead of Œ±, but as Œ± is a constant, then in order to simplify calculations, we'll just take 1/m‚ãÖŒ± to be our learning rate, and abuse the notation by just calling it Œ±\n",
    "\n",
    "So we have all the ingerdients to write Gradient Descent Algorithm ..\n",
    "\n",
    "In nut shell, we keep executing Gradient Step, n number times, until our error function is minimized or is zero, the n number of steps is called **Epochs**\n",
    "\n",
    "Now this look very similar to perceptron algorithm we wrote above .. we will study the differences soon ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Some helper functions for plotting and drawing lines\n",
    "\n",
    "def plot_points(X, y):\n",
    "    admitted = X[np.argwhere(y==1)]\n",
    "    rejected = X[np.argwhere(y==0)]\n",
    "    plt.scatter([s[0][0] for s in rejected], [s[0][1] for s in rejected], s = 25, color = 'blue', edgecolor = 'k')\n",
    "    plt.scatter([s[0][0] for s in admitted], [s[0][1] for s in admitted], s = 25, color = 'red', edgecolor = 'k')\n",
    "\n",
    "# See the Note for Display Function Below ..\n",
    "def display(m, b, color='g--'):\n",
    "    plt.xlim(-0.05,1.05)\n",
    "    plt.ylim(-0.05,1.05)\n",
    "    x = np.arange(-10, 10, 0.1)\n",
    "    plt.plot(x, m*x+b, color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I was puzzled how are we computing slope and intercept for line plotting, \n",
    "line has equation y = mx + c ..\n",
    "\n",
    "Found below explanation which makes all sense, if we refer code from training logic below where display is called, it will make things much clearer ..\n",
    "\n",
    "In Python, the indices start at 0, so the equation W1x1 + W2x2 + b = 0 is coded as\n",
    "\n",
    "W[0] x[0] + W[1] x[1] + b=0.\n",
    "\n",
    "In the graph, x[1] (i.e., x2) plays the role of y, and x[0] (i.e., x1), plays the role of x.\n",
    "\n",
    "Since matplotlib draws lines using the form y = mx + c, where m is the slope of the line and c is the y-intercept, we need to rewrite the line to this form, i.e., we rewrite Wx + b = 0 into the y = mx + c form, so we solve for x[1]:\n",
    "\n",
    "W[0] x[0] + W[1] x[1] + b = 0 ‚Äî> x[1] = (-W[0] / W[1]) x[0] + (-b / W[1])\n",
    "\n",
    "with the slope m = (-W[0] / W[1]) and the y-intercept c = (-b / W[1]). Now we can pass m and c to matplot lib who willl happily draw the line for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', header=None)\n",
    "X = np.array(data[[0,1]])\n",
    "y = np.array(data[2])\n",
    "\n",
    "plot_points(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implementing the basic functions\n",
    "Here is your turn to shine. Implement the following formulas, as explained in the text.\n",
    "- Sigmoid activation function\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "- Output (prediction) formula\n",
    "\n",
    "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "- Error function\n",
    "\n",
    "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$\n",
    "\n",
    "- The function that updates the weights\n",
    "\n",
    "$$ w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$\n",
    "\n",
    "$$ b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation (sigmoid) function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def output_formula(features, weights, bias):\n",
    "    return sigmoid(np.dot(features, weights) + bias)\n",
    "\n",
    "def error_formula(y, output):\n",
    "    return - y*np.log(output) - (1 - y) * np.log(1-output)\n",
    "\n",
    "def update_weights(x, y, weights, bias, learnrate):\n",
    "    output = output_formula(x, weights, bias)\n",
    "    d_error = -(y - output)\n",
    "    weights -= learnrate * d_error * x\n",
    "    bias -= learnrate * d_error\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "This function will help us iterate the gradient descent algorithm through all the data, for a number of epochs. It will also plot the data, and some of the boundary lines obtained as we run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(44)\n",
    "\n",
    "epochs = 100\n",
    "learnrate = 0.01\n",
    "\n",
    "def train(features, targets, epochs, learnrate, graph_lines=False):\n",
    "    \n",
    "    errors = []\n",
    "    n_records, n_features = features.shape\n",
    "    last_loss = None\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "    for e in range(epochs):\n",
    "        del_w = np.zeros(weights.shape)\n",
    "        for x, y in zip(features, targets):\n",
    "            output = output_formula(x, weights, bias)\n",
    "            error = error_formula(y, output)\n",
    "            weights, bias = update_weights(x, y, weights, bias, learnrate)\n",
    "        \n",
    "        # Printing out the log-loss error on the training set\n",
    "        out = output_formula(features, weights, bias)\n",
    "        loss = np.mean(error_formula(targets, out))\n",
    "        errors.append(loss)\n",
    "        if e % (epochs / 10) == 0:\n",
    "            print(\"\\n========== Epoch\", e,\"==========\")\n",
    "            if last_loss and last_loss < loss:\n",
    "                print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "            else:\n",
    "                print(\"Train loss: \", loss)\n",
    "            last_loss = loss\n",
    "            predictions = out > 0.5\n",
    "            accuracy = np.mean(predictions == targets)\n",
    "            print(\"Accuracy: \", accuracy)\n",
    "        if graph_lines and e % (epochs / 100) == 0:\n",
    "            display(-weights[0]/weights[1], -bias/weights[1])\n",
    "            \n",
    "\n",
    "    # Plotting the solution boundary\n",
    "    plt.title(\"Solution boundary\")\n",
    "    display(-weights[0]/weights[1], -bias/weights[1], 'black')\n",
    "\n",
    "    # Plotting the data\n",
    "    plot_points(features, targets)\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the error\n",
    "    plt.title(\"Error Plot\")\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.ylabel('Error')\n",
    "    plt.plot(errors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train the algorithm!\n",
    "When we run the function, we'll obtain the following:\n",
    "- 10 updates with the current training loss and accuracy\n",
    "- A plot of the data and some of the boundary lines obtained. The final one is in black. Notice how the lines get closer and closer to the best fit, as we go through more epochs.\n",
    "- A plot of the error function. Notice how it decreases as we go through more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(X, y, epochs, learnrate, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between Gradient Descent & Perceptron Algorithm\n",
    "\n",
    "Both have similar end goals and looks very similar, however the subte differences in operations are:\n",
    "\n",
    "1. Gradient Descent can take any values for $\\hat{y}$ vs Preceptron Algorithm can only take discrete values like 0 & 1.\n",
    "2. Preceptron Algorithm stops if all imputs are classified properly, it does not try to reduce error rate beyond, meaning if we have Blue Dot, in Blue Zone but on border line i.e. low accuracy, Preceptron Algorithm will not do anything, this is where Gradient Descent outshines, it will keep trying to optimize this Blue Dot to go father away from line to have hogher probaility and lower error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks \n",
    "\n",
    "We have enough Basics covered by this point to start understanding Neural Networks.\n",
    "\n",
    "Thus far we defined robust models which can hadle enough complexity discerte or continous, simple class or multi-class - however these are were all LINEAR in nature.\n",
    "\n",
    "In read world though, data is not always linearly seperable. We need curves!! And this where Neural Networks come in to the picture, idea is very simple, we take multiple linear models (Perceptrons), combine them to form non-linear outputs, and even further, we can combine non-linear models to even create more non-linearity in outputs. Below pictures hopefully justify the thought process.\n",
    "\n",
    "Combining two linear models (perceptrons)\n",
    "<img src=\"images/NN1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Representation as perceptron nodes\n",
    "<img src=\"images/NN2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Magic happens when we join the output of these as one model\n",
    "<img src=\"images/NN3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Simplifying above picture\n",
    "<img src=\"images/NN4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Simplifying output to just show final output\n",
    "<img src=\"images/NN5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Good segue to introduce ** NN Layers, as Input, Hidden, Output **\n",
    "<img src=\"images/NN6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Example of more than 2 perceptrons in hidden layer to compute more complex output\n",
    "<img src=\"images/NN7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Example of more complex input, more than 2 which takes in 3D Space, intermediate layes gives linear planes, and final output a non-linear 3D Space\n",
    "<img src=\"images/NN8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Good seque to introduce ** Deep Neural Networks **, as depicted below, we can have more than one hidden layer toicals compute more compex outputs\n",
    "<img src=\"images/NN9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "And Even Multiple Classes as output via same network\n",
    "<img src=\"images/NN10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Just more generic depiction of how much more complex it can get, most of the real world example are Deep NN with many hidden layers and many output layers\n",
    "<img src=\"images/NN12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Netwroks has 2 mains flows:\n",
    "\n",
    "## 1. Feedforward\n",
    "## 2. Backpropagation\n",
    "\n",
    "** Feedforward ** is something we have seen already in above description, we get end result as y_hat & error wrt target & for every input.\n",
    "\n",
    "** Backpropagation ** is understanding error rate, and applying the derivate of error rate wrt each node to compute revised Weights and Bias. \n",
    "\n",
    "We do this combination of Feedfoward & Backproparation n number of times (**epochs**) to get better model eventually.\n",
    "\n",
    "I will keep complexity of Chain Rule and computing Derivates away from this jupyter, and focus on end result and formulas which are important for calculation.\n",
    "\n",
    "**Feedforward View and Mathematical Equation**\n",
    "<img src=\"images/FF.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "**Backpropation View and Mathematical Equation, taking one example of W11 & going deeper on how to compute it, subseqently we have to do this for every node.**\n",
    "<img src=\"images/BP1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "<img src=\"images/BP2.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mean Squared Error Function\n",
    "\n",
    "So we studied about Log-Loss Function above, first we started deriving by finding distance of point from line, later we imprivised using Cross Entropy deriving error function to help optimize our models using Logistic Regression applying Gradient Descent\n",
    "\n",
    "There are other Error Functions, one of them is Mean Squared Error Function. As the name says, this one is the mean of the squares of the differences between the predictions and the labels.\n",
    "\n",
    "Obvious chioce for finding error or knowing how well we performed is: \n",
    "E = (y - $\\hat{y}$)\n",
    "\n",
    "However, problem with this equation is that if prediction is too high error will be negative, and if pediction is low, error wil be positive, we would rather keep the error be same by squaring the same ..\n",
    "\n",
    "E = (y - $\\hat{y}$)¬≤\n",
    "\n",
    "We could have alternatuvely used absolute, but squaring makes math nice also penalize outliers more than small errors.\n",
    "\n",
    "This is  error for one prediction though, and ideally we would like to know the error for whole network, improvising equation as ..\n",
    "\n",
    "E = 1/2 Œ£Œº(YŒº - $\\hat{Y}$Œº)¬≤\n",
    "\n",
    "We add 1/2 to make math better.\n",
    "\n",
    "This equation is called ** SSE - Sum of Squared Errors **\n",
    "\n",
    "** So if SSE is High Network is making bad preictions and if Low, Good Predictions **\n",
    "\n",
    "From before we saw that one weight update can be calculated as:\n",
    "\n",
    "Œîw·µ¢ = Œ∑Œ¥x·µ¢\n",
    "\n",
    "with the error term Œ¥ as\n",
    "\n",
    "Œ¥=(y‚àíy^)f‚Ä≤(h)=(y‚àíy^)f‚Ä≤(‚àëw·µ¢x·µ¢)\n",
    "\n",
    "Remember, in the above equation (y‚àíy^) is the output error, and f‚Ä≤(h) refers to the derivative of the activation function, f(h). We'll call that derivative the output gradient.\n",
    "\n",
    "Now I'll write this out in code for the case of only one output unit. We'll also be using the sigmoid as the activation function f(h). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"\n",
    "    # Derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5, 0.3, 0.1])\n",
    "\n",
    "### Calculate one gradient descent step for each weight\n",
    "### Note: Some steps have been consilated, so there are\n",
    "###       fewer variable names than in the above sample code\n",
    "\n",
    "# TODO: Calculate the node's linear combination of inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate the error term\n",
    "#       Remember, this requires the output gradient, which we haven't\n",
    "#       specifically added a variable for.\n",
    "error_term = error * sigmoid_prime(h)\n",
    "# Note: The sigmoid_prime function calculates sigmoid(h) twice,\n",
    "#       but you've already calculated it once. You can make this\n",
    "#       code more efficient by calculating the derivative directly\n",
    "#       rather than calling sigmoid_prime, like this:\n",
    "# error_term = error * nn_output * (1 - nn_output)\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = error_term * x\n",
    "\n",
    "# TODO: W' - Improvised Weights\n",
    "w_prime = w + (learnrate * del_w)\n",
    "print('Nodes Output (Linear Combination i.e. h) :')\n",
    "print(h)\n",
    "print(\"\")\n",
    "\n",
    "print('Neural Network output [Applying Activation Sigmoid(h) or y^] :')\n",
    "print(nn_output)\n",
    "print(\"\")\n",
    "\n",
    "print('Amount of Error (y - y^) :')\n",
    "print(error)\n",
    "print(\"\")\n",
    "\n",
    "print('Output Gradient f\\'(h)) :')\n",
    "print(sigmoid_prime(h))\n",
    "print(\"\")\n",
    "\n",
    "print('Error Term (Error * f\\'(h)) :')\n",
    "print(error_term)\n",
    "print(\"\")\n",
    "\n",
    "print('Change in Weights: [Delta W = learnrate * error_term * x]')\n",
    "print(del_w)\n",
    "print(\"\")\n",
    "\n",
    "print('W\\' - Improvised Weights [W + Delta W]:')\n",
    "print(w_prime)\n",
    "print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## continuing ... Mean Squared Error Function\n",
    "\n",
    "We're going to make a small change to how we calculate the error here. Instead of the **SSE**, we're going to use the **mean of the square errors (MSE)**. Now that we're using a lot of data, summing up all the weight steps can lead to really large updates that make the gradient descent diverge. To compensate for this, you'd need to use a quite small learning rate. Instead, we can just divide by the number of records in our data, mm to take the average. This way, no matter how much data we use, our learning rates will typically be in the range of 0.01 to 0.001. Then, we can use the MSE (shown below) to calculate the gradient and the result is the same as before, just averaged instead of summed.\n",
    "\n",
    "\n",
    "Equation:\n",
    "E = 1/2m Œ£Œº(YŒº - $\\hat{Y}$Œº)¬≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement network again using MSE and Numpy\n",
    "\n",
    "**Quick tip on initialization of weights:**\n",
    "\n",
    "We want these to be small such that the input to the sigmoid is in the linear region near 0 and not squashed at the high and low ends. It's also important to initialize them randomly so that they all have different starting values and diverge, breaking symmetry. So, we'll initialize the weights from a normal distribution centered at 0. A good value for the scale is 1/$\\sqrt{n}$ where nn is the number of input units. This keeps the input to the sigmoid low for increasing numbers of input units.\n",
    "\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "\n",
    "** Quick Note on Numpy**\n",
    "\n",
    "Numpy provides a function that calculates the dot product of two arrays, which conveniently calculates hh for us. The dot product multiplies two arrays element-wise, the first element in array 1 is multiplied by the first element in array 2, and so on. Then, each product is summed.\n",
    "\n",
    "-- input to the output layer\n",
    "output_in = np.dot(weights, inputs)\n",
    "\n",
    "With this let's dive into code, we will be using graduate school admissions data (found at http://www.ats.ucla.edu/stat/data/binary.csv). This dataset has three input features: GRE score, GPA, and the rank of the undergraduate school (numbered 1 through 4). Institutions with rank 1 have the highest prestige, those with rank 4 have the lowest.\n",
    "\n",
    "<img src=\"images/admissions-data.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "The goal here is to predict if a student will be admitted to a graduate program based on these features. For this, we'll use a network with one output layer with one unit. We'll use a sigmoid function for the output unit activation.\n",
    "\n",
    "** Data cleanup **\n",
    "You might think there will be three input units, but we actually need to transform the data first. The rank feature is categorical, the numbers don't encode any sort of relative values. Rank 2 is not twice as much as rank 1, rank 3 is not 1.5 more than rank 2. Instead, we need to use dummy variables to encode rank, splitting the data into four new columns encoded with ones or zeros. Rows with rank 1 have one in the rank 1 dummy column, and zeros in all other columns. Rows with rank 2 have one in the rank 2 dummy column, and zeros in all other columns. And so on.\n",
    "\n",
    "We'll also need to standardize the GRE and GPA data, which means to scale the values such they have zero mean and a standard deviation of 1. This is necessary because the sigmoid function squashes really small and really large inputs. The gradient of really small and large inputs is zero, which means that the gradient descent step will go to zero too. Since the GRE and GPA values are fairly large, we have to be really careful about how we initialize the weights or the gradient descent steps will die off and the network won't train. Instead, if we standardize the data, we can initialize the weights easily and everyone is happy.\n",
    "\n",
    "This is just a brief run-through, we will start with data clean up script first to achoeve something like below ..\n",
    "\n",
    "<img src=\"images/example-data.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Data Clean Up & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "print (\" Raw Data Excerpt ...\")\n",
    "print (\"\")\n",
    "print (admissions.head(5))\n",
    "\n",
    "print (\" Raw Data Statistics ...\")\n",
    "print (\"\")\n",
    "admissions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dummy variables for rank\n",
    "#pd.get_dummies - Create Dummy Columns with Appropriate Values based on Rank Column, Prefix Col names with Rank\n",
    "#pd.concat (combine prior columns with Dummy Columns)\n",
    "#data.drop Drop the Rank Column\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "print (\" Creating Dummy Columns from Rank (see explanation above for on why we are doing this) ...\")\n",
    "print (\"\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarize features\n",
    "# Look at mean and standard deviation for Columns GRE and GPA\n",
    "# Take Each Row for that Columns, Column_Value - Mean / Std to normlaize the data\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "\n",
    "print (\"Values passed to the random functions for understanding purposes...:\")\n",
    "print (\"\")\n",
    "print (\"data.index == Indices for all Values in data collection\")\n",
    "print (\"\")\n",
    "print (\"size (i.e. 90% of data) == {}\".format(int(len(data)*0.9)))\n",
    "print (\"\")\n",
    "\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "\n",
    "print (\"Sample Random Indexes...: {}\".format(sample[0:5]))\n",
    "print (\"\")\n",
    "\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "print (\"Two Collections Data (90% of data), Test_Data(10% of data), \\\n",
    "with each having # of records as: {}, {}\".format(data.shape[0],test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "#Simple enuogh to understand, column named admit is target as in 'y', \n",
    "# we will drop that as features for both data and test data\n",
    "\n",
    "print (\"Sample Features..\")\n",
    "print (\"\")\n",
    "print (features.head(5))\n",
    "\n",
    "\n",
    "print (\"Sample Targets..\")\n",
    "print (\"\")\n",
    "print (targets.head(5))\n",
    "\n",
    "print (\"Sample Test Features..\")\n",
    "print (\"\")\n",
    "print (features_test.head(5))\n",
    "\n",
    "\n",
    "print (\"Sample Test Targets..\")\n",
    "print (\"\")\n",
    "print (targets_test.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Build and Train Network\n",
    "\n",
    "Now that the data is ready, we see that there are six input features: gre, gpa, and the four rank dummy variables.\n",
    "\n",
    "We will use these to train our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 10000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "\n",
    "        # Activation of the output unit\n",
    "        #   Notice we multiply the inputs and the weights here \n",
    "        #   rather than storing h as a separate variable \n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # The error term\n",
    "        #   Notice we calulate f'(h) here instead of defining a separate\n",
    "        #   sigmoid_prime function. This just makes it faster because we\n",
    "        #   can re-use the result of the sigmoid function stored in\n",
    "        #   the output variable\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the \n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Percptrons\n",
    "\n",
    "Thus far we have build network with only one layer, now we will apply what we have to learn to build multi layer network, we will start with network with 1 Input Layer, 1 1 Hidden Layer and 1 Output Layer.\n",
    "\n",
    "Note though thus far we have been experimenting with single output, with exmaples going foward we will see more complex cases with output layer comoatining more than one outputs ..\n",
    "\n",
    "Key mathematical contruct we need to rememebr here us Matrices, Vectors and their multiplication principals.\n",
    "\n",
    "Without going in to too much detials, let's look at quick rules on Vector to Matrix multiplication as that is what we need to understand upcoming coding lab ..\n",
    "\n",
    "\n",
    "** Quick depliction of Weight indices with Hidden Layer in play **\n",
    "<img src=\"images/network-with-labeled-weights.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Before, we were able to write the weights as an array, indexed as w·µ¢\n",
    "\n",
    "But now, the weights need to be stored in a matrix, indexed as w${_i}{_j}$\n",
    "\n",
    "Each row in the matrix will correspond to the weights leading out of a single input unit, and each column will correspond to the weights leading in to a single hidden unit. For our three input units and two hidden units, the weights matrix looks like this:\n",
    "\n",
    "<img src=\"images/multilayer-diagram-weights.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Be sure to compare the matrix above with the diagram shown before it so you can see where the different weights in the network end up in the matrix.\n",
    "\n",
    "To initialize these weights in Numpy, we have to provide the shape of the matrix. If features is a 2D array containing the input data:\n",
    "\n",
    "\\# Number of records and input units<br>\n",
    "n_records, n_inputs = features.shape<br>\n",
    "\\# Number of hidden units<br>\n",
    "n_hidden = 2<br>\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "This creates a 2D array (i.e. a matrix) named weights_input_to_hidden with dimensions n_inputs by n_hidden. Remember how the input to a hidden unit is the sum of all the inputs multiplied by the hidden unit's weights. So for each hidden layer unit, h${_j}$, we need to calculate the following:\n",
    "<br>\n",
    "<img src=\"images/hidden-layer-weights.gif\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "n this case, we're multiplying the inputs (a row vector here) by the weights. To do this, you take the dot (inner) product of the inputs with each column in the weights matrix. For example, to calculate the input to the first hidden unit, j=1, you'd take the dot product of the inputs with the first column of the weights matrix, like so:\n",
    "\n",
    "<img src=\"images/input-times-weights.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "And for the second hidden layer input, you calculate the dot product of the inputs with the second column. And so on and so forth.\n",
    "\n",
    "In Numpy, you can do this for all the inputs and all the outputs at once using np.dot\n",
    "\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "\n",
    "You could also define your weights matrix such that it has dimensions n_hidden by n_inputs then multiply like so where the inputs form a column vector:\n",
    "\n",
    "\n",
    "<img src=\"images/inputs-matrix.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "Note: The weight indices have changed in the above image and no longer match up with the labels used in the earlier diagrams. That's because, in matrix notation, the row index always precedes the column index, so it would be misleading to label them the way we did in the neural net diagram. Just keep in mind that this is the same weight matrix as before, but rotated so the first column is now the first row, and the second column is now the second row.\n",
    "\n",
    "**Making a column vector**\n",
    "You see above that sometimes you'll want a column vector, even though by default Numpy arrays work like row vectors. It's possible to get the transpose of an array like so arr.T, but for a 1D array, the transpose will return a row vector. Instead, use arr[:,None] to create a column vector:\n",
    "\n",
    "print(features)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features.T)\n",
    "> array([ 0.49671415, -0.1382643 ,  0.64768854])\n",
    "\n",
    "print(features[:, None])\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "       \n",
    "Alternatively, you can create arrays with two dimensions. Then, you can use arr.T to get the column vector.\n",
    "\n",
    "np.array(features, ndmin=2)\n",
    "> array([[ 0.49671415, -0.1382643 ,  0.64768854]])\n",
    "\n",
    "np.array(features, ndmin=2).T\n",
    "> array([[ 0.49671415],\n",
    "       [-0.1382643 ],\n",
    "       [ 0.64768854]])\n",
    "       \n",
    "       \n",
    "I personally prefer keeping all vectors as 1D arrays, it just works better in my head.\n",
    "\n",
    "**Programming Lab**\n",
    "Below, we'll implement a forward pass through a 4x3x2 network, with sigmoid activation functions for both layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "hidden_layer_in = np.dot(X,weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cont... Backpropagation\n",
    "\n",
    "Wonderful, thus far we build feed foward multi-layer (2 layer in this case) network.\n",
    "\n",
    "Now, it is time to compute error rate, as we have done before, with difference that error rate has to be compute for each layer & each node for that layer, compute optimized weights. Run the network for n epochs until we minimize the error rate to satisfaction level.\n",
    "\n",
    "This whole is process is something we already know is called Backpropagation.\n",
    "\n",
    "Since we have multiple layers in play - let's go through quick example to understand this prior we code the same ..\n",
    "\n",
    "Let's walk through the steps of calculating the weight updates for a simple two layer network. Suppose there are two input values, one hidden unit, and one output unit, with sigmoid activations on the hidden and output units. The following image depicts this network. (Note: the input values are shown as nodes at the bottom of the image, while the networks output value is shown as y^ at the top. The inputs themselves do not count as a layer, which is why this is considered a two layer network.)\n",
    "\n",
    "<img src=\"images/backprop-network.png\" alt=\"Drawing\" style=\"width: 150px;\"/>\n",
    "\n",
    "<img src=\"images/BProp.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "Time to code!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with how to caluclate the error for output and hidden layers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have calculated the errors for each layer, let's build out backpropagation on schools graduation data we used earlier..\n",
    "\n",
    "We are taking simple network of one hidden layer & one output.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from data_prep import features, targets, features_test, targets_test\n",
    "\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 3000\n",
    "learnrate = 0.01\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                        size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        # TODO: Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        # TODO: Calculate the network's prediction error\n",
    "        error = y - output\n",
    "\n",
    "        # TODO: Calculate error term for the output unit\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "\n",
    "        # TODO: Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "\n",
    "        # TODO: Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        # TODO: Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # TODO: Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop is really really important, even though higher level frameworks like Tensor will do these for us, it is very important to understand the fundamentals. Some additions links to study further ..\n",
    "\n",
    "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b\n",
    "\n",
    "https://www.youtube.com/watch?v=59Hbtz7XgjM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimzation Techniques\n",
    "\n",
    "Now, we have learned basics of NN end-to-end, it may be obvious that success heavily depends on varity of hyperparameters. And hence we have to understand & layout some foundational guidelines/techniques which can help us stay on track achieving desired results, we will refer these techniques as optimization guidelines.\n",
    "\n",
    "Prioe we dig deep with techniques, let's understand the various patterns and problems which would require better knowledge of these.\n",
    "\n",
    "### Patterns/Problems:\n",
    "\n",
    "**1. Overfitting (High Variance) **- Overfitting is the state when model errors rate is very low on training set, but it does not perform very well on generic data set, i.e. we have trained our model too much specific to a dataset. Example, we are able to classify dogs in brown, white furs, ut any other color of dogs are misclassified. This is also indication that our model is too complex with many layers ..\n",
    "\n",
    "** 2. Underfitting (High Bias) **- Underfitting is the state when trained model is too loose or not trained enough, or way too generic in nature, or training error and testing errors are high. It is able to classify but with lot of mistakes & ignoring subtle boundaries which provides relevance to the subject.\n",
    "\n",
    "> **Mitigations (Overfitting / Underfitting) **\n",
    "\n",
    ">>**1. Early Stopping ( for Under and Overfitting both ) **\n",
    "So what is the right state?? It is not an easy answer, one way one can perceive the optimal state but observing errors between **traning and test data**, so far we did not talk about training abd test data, we just refered data as input data. What we do is split input data to 2 buckets called training and test, we use training data to train the model, model never see the test data. Once we are happy with state that trainin is optimal we use the optimized model on test data.\n",
    "\n",
    ">>With clarity on what test and training data buckets are: Below diagram show observations on these for n number of epochs, we can see that state which is referred as just right, has test data error reached to a minimum point called **Goldilocks Spot** and start to increase from there, this is point we start to tend towards overfitting, i.e. spots generalizing and starts memorizing.\n",
    "\n",
    "<img src=\"images/UOFitting.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">>This plot is also called ** Model Complexity Graph **\n",
    "\n",
    "<img src=\"images/CGraph.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    ">>**2. Regularization (Overfitting)**\n",
    "\n",
    ">>Regulirazation is another technique we apply to mitigate overfitting, essence is to penalize lager weights. It is taking the old error function we had and adding either lamba(sum of weights) or  lamba(sum of weights squared).\n",
    "\n",
    "<img src=\"images/regularization.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    ">>When to use L1 vs L2 ??\n",
    "\n",
    ">>Both have different Sparsity rate as output. \n",
    "\n",
    ">>**L1 Regularization ** produce more sparse vectors, so if we want to reduce number of weights & end up with small set we can use L1. It is also good for feature selection, sometimes we have hundreds of features, and with L1 technique we can select the ones whcih are important & turn rest in to zeros.\n",
    "\n",
    ">>**L2 Regularization ** on the other hand, tends not to favor sparse veactors since it tries to maintain all the weights homogeneously small. This one normally gives better results for training models so it's the one we will use the most.\n",
    "\n",
    "\n",
    ">>Intution of these techniques using an example below:\n",
    "<img src=\"images/regularization1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">> Cheat Sheet:\n",
    "<img src=\"images/regularization2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">>**3. Dropouts (Overfitting)**\n",
    "\n",
    ">>This is another simple yet effective and very popular technique. What we do here is that define probability of each node be dropped by certain percentage. The reasoning behind this is that there are nodes with heavy weights, and they start to influence outcome of the network, when we drop each nodes in this manner, every node get fair share to influence the outcome. What we do is define probability like 20%, which means each node gets dropped 20% of the time. \n",
    "\n",
    "** 3. Vanishing Gradient **\n",
    "Above two links to go in depth talk about Backpropagation spot light issues like Vanishing Graient. In summary, if we look at sigmoid function below, at the extreme left and right outcomes are pretty much flat, which means the gradient is almost near zero ..\n",
    "\n",
    "<img src=\"images/vg1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Just to recall, Backpropagation apply chain rule, which means these gradient gets multiplied, and have overall very tiny effect, or in other words we move super tiny steps.\n",
    "\n",
    "<img src=\"images/vg2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "As repurcussion, this makes training difficult, as Gradient Descent makes very small changes to weights, in other words, gradient descent takes very very tiny steps and may never reach at the bottom of the mount everest as shown below ..\n",
    "\n",
    "<img src=\"images/vg3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">** Mitigations (Vanishing Graient, Use different activation functions)**\n",
    ">>** 1. tanh :** This activation function though very similat to sigmoid has lager derivatives which provide great advances to NN.\n",
    "    \n",
    "<img src=\"images/tanh.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "    \n",
    ">> **2. ReLU (Rectified Linear Units):** This activation function very simple, it says, if you are positive, I will return same value, if you are negative I will return 0. Another way  to see ouput is between x & xero. This function is used a lot instead of sigmoid, as it can dramatically improve the performace of the network without sacrificing much accuracy. Since it's derivative is 1 for positive numbers. It is fascinating that this function which barely breaks linearilty can lead to such complex non-linear solutions.\n",
    "\n",
    ">>Below is example of multi-layer network with bunch of ReLU activation units (shown as relu graph symbol). Note that the last unit is sigmoid, as we still need final output as probabiliy between zero and one.However, if we let the end up units being ReLU we can end up with regression models, as the predictive value. We will see more during RNN segment ..\n",
    "    \n",
    "<img src=\"images/relu.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/relu1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "    \n",
    "** 4. Local Minima **\n",
    "During Gradient Descent, there could be mutiple slopes however there is only one overall minimum and rest are defined as local minimum. If process reach at local minimum it may get stuck there and may not see option to process in any direction further. Mitigations - TBD\n",
    "\n",
    "<img src=\"images/lm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">** Mitigations **\n",
    "\n",
    ">> **1. Random Restart** : One way to solve this is to restart at a few different random places. This increases probability that we get to the global minimum or at least a pretty good local minimum.\n",
    "\n",
    "<img src=\"images/lm1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">> **2. Increase Momentum**: Idea here is to increase the momentum to push us over local minima and have the process not get stuck here rather progress towards global minimum. This can be achieved but not just taking gradient of last step but average of last few steps, we also know that last step is most relevant and the ones prior are not that much relevant, so we introduce new hyper-parameter called Beta (Momentum), this factor is nothing but weighing how much each step should influence our decision, and if you see equation below, Beta for prior steps decrease the inlfuence as it is applied as multinomial. Having said this, it will drift us away from the Global Minimum a bit but not that much --- theoritically it seems vague on why this would work to get us over local minimum and why not global minimum but in practice it works pretty well.\n",
    "\n",
    "<img src=\"images/momentum.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** 5. Gradient Descent vs Stochastic Gradient Descent **\n",
    "\n",
    "**Gradient Descent** we take one step with all the data and try to acheieve overall minima. The cost we pay is huge memory footprint and massive computational power.\n",
    "\n",
    "SGD in contrast, subset the data in n bathes and do n number of passes on each batch of data, which may not be as accurate as GD but it does take us in right direction progressively. The advantage here is that we can get sense of direction early on and also cost wrt memory and computational power neded is much less, offcourse we will apply SGD on overall data eventually (instead of one go & in batches).\n",
    "\n",
    "** 6. Learning Rate Decay **\n",
    "\n",
    "What learning rate to use, if we use high learning rate, we may miss local minima, however if we use small learning rate, we slow down the process.\n",
    "\n",
    "<img src=\"images/lr.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "*Best learning rate is the one whcih decrease as model gets closer to the solution.* . We will see later that Keras has some options to let us do this.\n",
    "\n",
    "<img src=\"images/lr1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "** 7. Error Functions **: Lastly, there are various choice of error functions which made the list but we did not study. However is the list for reference purposes:\n",
    "- Mount Errorest (Studied)\n",
    "- Mount Kilimanjerror (Studied)\n",
    "- Mount Reinerror\n",
    "- Mount Ves-Oops-Vius\n",
    "- EyjafJallaJoKull / EyjafVillaJoKull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics \n",
    "\n",
    "Now that we know how to train neural networks, and how to optimize their training,, we need to know how to evaluate their behavior properly. For this, we use certain very useful metrics. In this section, we'll learn some of the most common metrics used in deep learning, and also in machine learning in general.\n",
    "\n",
    "### 1. Confusion Matrix - Tells us how good our model is..\n",
    "\n",
    "<img src=\"images/em1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Type 1 and Type 2 Errors**\n",
    "Sometimes in the literature, you'll see False Positives and True Negatives as Type 1 and Type 2 errors. Here is the correspondence:\n",
    "\n",
    "Type 1 Error (Error of the first kind, or False Positive): In the medical example, this is when we misdiagnose a healthy patient as sick.\n",
    "Type 2 Error (Error of the second kind, or False Negative): In the medical example, this is when we misdiagnose a sick patient as healthy.\n",
    "\n",
    "### 2. Accuracy - Reflects how many inputs did we classify correctly ..\n",
    "\n",
    "A = Correctly Classfied Points / All Inputs\n",
    "\n",
    "Code Hint: sklearn has library to compute this (see image below)\n",
    "\n",
    "<img src=\"images/em4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "**Accuracy is not always a good measure**, look at example below, model lables are such that perdicts all good ones as good correct and also predicts fraud transactions as correct, but if we see ratio model safely says all transactions are good with high accuracy but problem is that it does not catch any bad ones .. so if data is skewed like this one, it is not good to use accuracy score.\n",
    "\n",
    "<img src=\"images/em5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "It is the same case if we lable all records as bad, in this case we are catching all good ones!\n",
    "\n",
    "<img src=\"images/em6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### 3. Problem with False Negatives and Positives\n",
    "\n",
    "Which one is more worse, actually it depends on the use case, let's look at the two examples we are dealing with below ..\n",
    "\n",
    "<img src=\"images/em7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### 4. Precison and Recall (As Mitigation to Problem at #3)\n",
    "\n",
    "<img src=\"images/em9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "** Let's look at Precision first .. **\n",
    "<img src=\"images/em10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"images/em12.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em13.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "** Now let's look at recalls **\n",
    "\n",
    "<img src=\"images/em14.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em15.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<img src=\"images/em16.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/em17.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### 5. ROC Curve (Receiver Operating Curve)\n",
    "\n",
    "Basically we split the data, below given example is one dimensional data with 3 different split possibilities based on data.\n",
    "\n",
    "What we do is calculate True Positive rate and False Positive Rate (formulas given below) and plot these for each observations. Observation area for most perfect is Square with Area as 1.\n",
    "\n",
    "Intutions below:\n",
    "<img src=\"images/rc1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/rc8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION \n",
    "\n",
    "So far we looked at Classification cases, now we will study Regression which answeres how much, how much does this house cost etc\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Looks at the data and draws the best fitting line. Using this line we can network make predicions.[ Remember Logitics Regression we studied above, which outputs 0 or 1 as output instead of numeric values as Linear Regression produces, subtle difference in outcomes]\n",
    "\n",
    "<img src=\"images/lr2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "## So how we draw the best fitting line?? **\n",
    "\n",
    "Just a quick refresher on how line equation, notion of Slope and Intercept. And how these impact movement of line.\n",
    "\n",
    "W1 denotes Slope, changes in same contributing to line slope, <br>\n",
    "W2 denotes Intercept, chnages in same will result parallel movement\n",
    "\n",
    "Some examples below ..\n",
    "\n",
    "<img src=\"images/lr3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/lr4.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/lr5.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/lr6.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/lr7.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "### 1. Absolute Trick : On how to move line faster towards point **\n",
    "\n",
    "Something we have seen earlier too. So we will not detail too much.\n",
    "\n",
    "If point(p,q) is above line, we will add p from slope and 1 from intercept, we will have multiplier alpha (learning rate) for gradual increase or decrease. \n",
    "\n",
    "<img src=\"images/lr8.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "If point(p,q) is below line, we will subtract p from slope and 1 from intercept, we will have multiplier alpha (learning rate) for gradual increase or decrease. \n",
    "\n",
    "<img src=\"images/lr9.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### 2. Square Trick : On how to move line faster towards point **\n",
    "\n",
    "This is very similar to Absolute Trick, only difference is that this trick is aware of how far the point is from line, ie. it also knows the vertical distance between line and point. In contrast Absolute Trick was not aware of how far we are from line, it just knew horizontal distance and was working purely based on that.\n",
    "\n",
    "This makes change makes a great positive difference, as now the equation will move coiefficients larger for larger distances and smaller for smaller distances. \n",
    "\n",
    "<img src=\"images/lr10.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Something, we get for free is that this technique serve for points below line without any change as p-p' will be negative. (For Absolute trick, we have separate rules/equations for point above and below line)\n",
    "\n",
    "<img src=\"images/lr11.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "### 3. Gradient Descent Minimizing Error Function\n",
    "We already know what Gradient Descent is, so I will skip any explanation. We will be using Gradient Descent as for Classification case to iterarte over network minimizing the overall error rate.\n",
    "\n",
    ">#### Error Functions\n",
    ">There are two most common error functions for Linear Regression, let's study both ..\n",
    "\n",
    ">**3.1 Mean Absolute Error**\n",
    "\n",
    "As studied before, we will take difference of distance from perdiction and target to understand error for single point. Subsequently, we will compute the same for all points and take mean of these. We have 1/m for our conveience, as will take derivatives of this error later simplifying the math. Also notice we have absolute of y-y_hat -- with linear regression we have two situation point above line or below, generating positive and -ve errors -- this would warrant us to keep our error positive to not be cancelled out negative errors and hence use of absolute.\n",
    "\n",
    "<img src=\"images/mae.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">**3.2 Mean Squared Error**\n",
    "\n",
    "In this case, instead of verticle distance, we will instead take area from point and perdiction. Subseuqnetly taking avregae of these for whole set. We do not have absolute here, but notice we have square of errors whcih will eventually give positive error only. And as prior, we have 1/m for our conveience, as will take derivatives of this error later & it help make match easy.\n",
    "\n",
    "<img src=\"images/mse.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "## In Summary we studied 4 tricks to best fit line in data ..\n",
    "\n",
    "1. Absolute Trick\n",
    "2. Square trick\n",
    "3. MAE - Mean Absolute Error applying Gradient Descent\n",
    "4. MSE - Mean Squared Error applying Gradient Descent\n",
    "\n",
    "It is interesting to see but <br>\n",
    "\n",
    "**MAE == Absolute Trick** as each Gradient Step in MAE is nothing but Absolute Trick. Same goes for ..\n",
    "\n",
    "**MSE == Square Trick** as each Gradient Step in MSE is nothing but Square Trick.\n",
    "\n",
    "[ Skipping mathematical details ]\n",
    "\n",
    "\n",
    "## Some fundamental Q's\n",
    "\n",
    "> ** 1. MAE vs MSE ??? **\n",
    "\n",
    ">A potential confusion is the following: How do we know if we should use the mean or the total squared (or absolute) error?\n",
    "\n",
    ">The good news is, it doesn't really matter. As we can see, the total squared error is just a multiple of the mean squared error, since\n",
    "\n",
    ">M = mT.\n",
    "\n",
    ">Therefore, since derivatives are linear functions, the gradient of T is also m m times the gradient of M.\n",
    "\n",
    ">However, the gradient descent step consists of subtracting the gradient of the error times the learning rate \\alpha Œ±. Therefore, choosing between the mean squared error and the total squared error really just amounts to picking a different learning rate.\n",
    "\n",
    ">In real life, we'll have algorithms that will help us determine a good learning rate to work with. Therefore, if we use the mean error or the total error, the algorithm will just end up picking a different learning rate.\n",
    "\n",
    "\n",
    ">  ** 2. Batch vs Stochastic Gradient Descent vs Mini-Batch Gradient Descent??? **\n",
    "\n",
    ">At this point, it seems that we've seen two ways of doing linear regression.\n",
    "\n",
    ">By applying the squared (or absolute) trick at every point in our data one by one, and repeating this process many times.\n",
    "By applying the squared (or absolute) trick at every point in our data all at the same time, and repeating this process many times.\n",
    "More specifically, the squared (or absolute) trick, when applied to a point, gives us some values to add to the weights of the model. We can add these values, update our weights, and then apply the squared (or absolute) trick on the next point. Or we can calculate these values for all the points, add them, and then update the weights with the sum of these values.\n",
    "\n",
    ">The latter is called batch gradient descent. The former is called stochastic gradient descent.\n",
    "\n",
    "<img src=\"images/bs.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    ">The question is, which one is used in practice?\n",
    "\n",
    ">Actually, in most cases, neither. Think about this: If your data is huge, both are a bit slow, computationally. The best way to do linear regression, is to split your data into many small batches. Each batch, with roughly the same number of points. Then, use each batch to update your weights. This is still called mini-batch gradient descent.\n",
    "\n",
    "<img src=\"images/mb.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "> **3. Absolute Error vs Squared Error **\n",
    "\n",
    ">There is no clear answer but both have one distinct difference. If we look at scenario below, MAE has no change with line movement but MSE has clear distinction on which line has lowest error rate. MSE will have edge drawing the most optimal line, though it is subjective that every case may not demand the same.\n",
    "\n",
    "<img src=\"images/err1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "<img src=\"images/err2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Time to code!!\n",
    "\n",
    "We will be scikit-learn library .. \n",
    "\n",
    "In this section, you'll use linear regression to predict life expectancy from body mass index (BMI). Before you do that, let's go over the tools required to build this model.\n",
    "\n",
    "For your linear regression model, you'll be using scikit-learn's LinearRegression class. This class provides the function fit() to fit the model to your data.\n",
    "\n",
    ">>> from sklearn.linear_model import LinearRegression\n",
    ">>> model = LinearRegression()\n",
    ">>> model.fit(x_values, y_values)\n",
    "\n",
    "In the example above, the model variable is a linear regression model that has been fitted to the data x_values and y_values. Fitting the model means finding the best line that fits the training data. Let's make two predictions using the model's predict() function.\n",
    "\n",
    ">>> print(model.predict([ [127], [248] ]))\n",
    "[[ 438.94308857, 127.14839521]]\n",
    "\n",
    "The model returned an array of predictions, one prediction for each input array. The first input, [127], got a prediction of 438.94308857. The seconds input, [248], got a prediction of 127.14839521. The reason for predicting on an array like [127] and not just 127, is because you can have a model that makes a prediction using multiple features. We'll go over using multiple variables in linear regression later in this lesson. For now, let's stick to a single value.\n",
    "\n",
    "Data samples are from https://www.gapminder.org/ & data file include three columns:\n",
    "\n",
    "    Country ‚Äì The country the person was born in.\n",
    "    Life expectancy ‚Äì The average life expectancy at birth for a person in that country.\n",
    "    BMI ‚Äì The mean BMI of males in that country.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assign the dataframe to this variable.\n",
    "bmi_life_data = pd.read_csv(\"bmi_and_life_expectancy.csv\")\n",
    "\n",
    "print (\"Raw data Samples ...\")\n",
    "print (bmi_life_data.head(5))\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "#Set features to BMI\n",
    "X = bmi_life_data[['BMI']]\n",
    "\n",
    "#Set target as Life Expectancy\n",
    "y = bmi_life_data[['Life expectancy']]\n",
    "\n",
    "print(\"Features and Target Dimesions are: \", X.shape, y.shape)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "bmi_life_model = LinearRegression()\n",
    "bmi_life_model.fit(X, y)\n",
    "\n",
    "# Mak a prediction using the model\n",
    "# TODO: Predict life expectancy for a BMI value of 21.07931\n",
    "laos_life_exp = bmi_life_model.predict(21.07931)\n",
    "\n",
    "print(\"Perdiction Result for BMI: 21.07931 is {}\".format(laos_life_exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Dimesions\n",
    "\n",
    "So we dealt with two-dimensional data, how do we deal with higher dimensions say 3 dimensions. We have studied this for classification and it is no different here. Instead of a line, we will have plane and our equation will improvise as:\n",
    "\n",
    "<img src=\"images/hd1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Similarly, we will be dealing with nth dimesions as below ..\n",
    "\n",
    "<img src=\"images/hd2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Time to code Multiple Linear Regression.\n",
    "\n",
    "In this quiz, we'll be using the Boston house-prices dataset. The dataset consists of 13 features of 506 houses and their median value in $1000's. You'll fit a model on the 13 features to predict on the value of houses.\n",
    "\n",
    "Boston house-prices dataset : https://archive.ics.uci.edu/ml/machine-learning-databases/housing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Gradient Descent & Not use Linear Alegbra ( Closed Form Math Solution to Minimize the Error )\n",
    "\n",
    "<img src=\"images/cfm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "As shown above we can surely solve using linear algebra values of Weights which will minimize the error. However this  method will be expensive in real life, since finding the inverse of the matrix X$^{T}$X is hard, if n is large. \n",
    "\n",
    "That's why we go through the pain of doing gradient descent many times. But if our data is sparse, namely, if most of the entries of the matrix X are zero, there are some very interesting algorithms which will find this inverse quickly, and that'll make this method useful in real life.\n",
    "\n",
    "\n",
    "## Linear Regression Warnings\n",
    "\n",
    "Linear regression comes with a set of implicit assumptions and is not the best model for every situation. Here are a couple of issues that you should watch out for.\n",
    "\n",
    "**Linear Regression Works Best When the Data is Linear**\n",
    "Linear regression produces a straight line model from the training data. If the relationship in the training data is not really linear, you'll need to either make adjustments (transform your training data), add features (we'll come to this next) or use another kind of model.\n",
    "\n",
    "<img src=\"images/quadraticlinearregression.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "**Linear Regression is Sensitive to Outliers**\n",
    "Linear regression tries to find a 'best fit' line among the training data. If your dataset has some outlying extreme values that don't fit a general pattern, they can have a surprisingly large effect.\n",
    "\n",
    "In this first plot, the model fits the data pretty well.\n",
    "\n",
    "<img src=\"images/lin-reg-no-outliers.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "However, adding a few points that are outliers and don't fit the pattern really changes the way the model predicts.\n",
    "\n",
    "<img src=\"images/lin-reg-w-outliers.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In most circumstances, you'll want a model that fits most of the data most of the time, so watch out for outliers!\n",
    "\n",
    "## So how do we solve above problem i.e. if data is non-linear, something like below ??\n",
    "\n",
    "** 1. Polynomial Regression : **\n",
    "<img src=\"images/poly.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Instead of line, we will need curve, and this can be solved using higher degree polynomials, hence this approach is called ** Polynomial Regression **\n",
    "\n",
    "> **Regularization** is nice technique to avoid over-fitting and it works for both Classification as well as Regression Models. \n",
    "\n",
    ">Will not go over any more details here, refer to Clasification section for details on this topic.\n",
    "\n",
    "** 2. Neural Network Regression ( Piecewise Linear Functions ) **: This is another way to deal with non-linear data, i.e. combine various linear combination in pieces to derive final output which is non-linear. \n",
    "\n",
    "<img src=\"images/nnr.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can use Neural Networks to solve this, below is example of NN we used for classification problem, it is exact same network with only change as last node, which is not sigmoid ( as sigmoid only returns 0 or 1), once we remove this, model start returning any number!! \n",
    "\n",
    "Error Function would go under slight change, we will use MSE or MAE what we learned in regression, once we couple this with Backprop, we will be able to train our network as classification !!\n",
    "\n",
    "<img src=\"images/nnr1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We can use other activation functions like tanh/sigmoid < not clear on how to use these for regression >\n",
    "\n",
    "So in summary NN can be used for both Classification and Regression with slight to final activation functions.\n",
    "\n",
    "\n",
    "### Visual and Interactive Guidel to NN : http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Code !!\n",
    "\n",
    "We will be doing detailed lab for NN - Regression and hence follow the link below for working example...\n",
    "\n",
    "https://github.com/anshoomehra/udacity-deep-learning/tree/master/first-neural-network\n",
    "\n",
    "Study file name : Your_first_neural_network.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
