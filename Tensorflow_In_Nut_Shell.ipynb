{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent to use this jupyter for myself, quick refresher / class-notes for course material studied for  Neural Networks from Udacity [ & not to reproduce any material as such ..]\n",
    "\n",
    "This guide DOES NOT go in depth explaining fundamentals, & may appear be in arbitraty sequence to follow, as it is aligned to class labs. One should go through proper course material prior using this guide. I highly recommend Udacity nanodegree course(s) as right medium to learn this subject.\n",
    "\n",
    "Content Credit: Udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello World!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create TensorFlow object called hello_constant\n",
    "hello_constant = tf.constant('Hello World!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Run the tf.constant operation in the session\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's understand what is going under the hood\n",
    "\n",
    "**Tensor**\n",
    "\n",
    "In TensorFlow, data isn’t stored as integers, floats, or strings. These values are encapsulated in an object called a tensor. In the case of hello_constant = tf.constant('Hello World!'), hello_constant is a 0-dimensional string tensor, but tensors come in a variety of sizes as shown below:\n",
    "\n",
    "1. A is a 0-dimensional int32 tensor <br>\n",
    "A = tf.constant(1234) \n",
    "2. B is a 1-dimensional int32 tensor <br>\n",
    "B = tf.constant([123,456,789]) \n",
    "3. C is a 2-dimensional int32 tensor <br>\n",
    "C = tf.constant([ [123,456,789], [222,333,444] ])\n",
    "\n",
    "tf.constant() is one of many TensorFlow operations you will use in this lesson. The tensor returned by tf.constant() is called a constant tensor, because the value of the tensor never changes.\n",
    "\n",
    "**Session**\n",
    "\n",
    "TensorFlow’s api is built around the idea of a computational graph, a way of visualizing a mathematical process which you learned about in the MiniFlow lesson. Let’s take the TensorFlow code you ran and turn that into a graph:\n",
    "\n",
    "<img src=\"images/session.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "A \"TensorFlow Session\", as shown above, is an environment for running a graph. The session is in charge of allocating the operations to GPU(s) and/or CPU(s), including remote machines. Let’s see how you use it.\n",
    "\n",
    ">with tf.Session() as sess:<br>\n",
    ">>output = sess.run(hello_constant)<br>\n",
    ">>print(output)\n",
    "\n",
    "The code has already created the tensor, hello_constant, from the previous lines. The next step is to evaluate the tensor in a session.\n",
    "\n",
    "The code creates a session instance, sess, using tf.Session. The sess.run() function then evaluates the tensor and returns the results.\n",
    "\n",
    "After you run the above, you will see the following printed out:\n",
    "\n",
    ">'Hello World!'\n",
    "\n",
    "\n",
    "What if you want to use a non-constant? This is where tf.placeholder() and feed_dict come into place.\n",
    "\n",
    "** tf.placeholder() **\n",
    "\n",
    "Sadly you can’t just set x to your dataset and put it in TensorFlow, because over time you'll want your TensorFlow model to take in different datasets with different parameters. You need tf.placeholder()!\n",
    "\n",
    "\n",
    "tf.placeholder() returns a tensor that gets its value from data passed to the tf.session.run() function, allowing you to set the input right before the session runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Hello World'})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. The above example shows the tensor x being set to the string \"Hello, world\". It's also possible to set more than one tensor using feed_dict as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(y, feed_dict={x: 'Test String', y: 123, z: 45.67})\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the data passed to the feed_dict doesn’t match the tensor type and can’t be cast into the tensor type, you’ll get the error “ValueError: invalid literal for...”.\n",
    "\n",
    "** Let's look at Arithmetic Functions **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.add(5, 2)  # 7\n",
    "y = tf.subtract(10, 4) # 6\n",
    "z = tf.multiply(2, 5)  # 10\n",
    "x = tf.divide(6.0,2.0) #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting types**\n",
    "It may be necessary to convert between types to make certain operators work together. For example, if you tried the following, it would fail with an exception:\n",
    "\n",
    "\n",
    "tf.subtract(tf.constant(2.0),tf.constant(1))  \n",
    "\n",
    "// Fails with ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32:\n",
    "\n",
    "\n",
    "That's because the constant 1 is an integer but the constant 2.0 is a floating point value and subtract expects them to match.\n",
    "\n",
    "\n",
    "In cases like these, you can either make sure your data is all of the same type, or you can cast a value to another type. In this case, converting the 2.0 to an integer before subtracting, like so, will give the correct result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sub_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# Another Example \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "s = tf.divide(x,y)\n",
    "#z = tf.subtract(s,1)  This works as well but better way is\n",
    "z = tf.subtract(s, tf.cast(tf.constant(1), tf.float64))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(z)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear functions in TensorFlow**\n",
    "\n",
    "The most common operation in neural networks is calculating the linear combination of inputs, weights, and biases. As a reminder, we can write the output of the linear operation as\n",
    "\n",
    "y = xW + b\n",
    "\n",
    "Here, W is a matrix of the weights connecting two layers. The output y, the input x, and the biases b are all vectors.\n",
    "\n",
    "\n",
    "**Weights and Bias in TensorFlow**\n",
    "The goal of training a neural network is to modify weights and biases to best predict the labels. In order to use weights and bias, you'll need a Tensor that can be modified. This leaves out tf.placeholder() and tf.constant(), since those Tensors can't be modified. This is where tf.Variable class comes in.\n",
    "\n",
    "tf.Variable()\n",
    "\n",
    "\n",
    "x = tf.Variable(5)\n",
    "\n",
    "The tf.Variable class creates a tensor with an initial value that can be modified, much like a normal Python variable. This tensor stores its state in the session, so you must initialize the state of the tensor manually. You'll use the tf.global_variables_initializer() function to initialize the state of all the Variable tensors.\n",
    "\n",
    "\n",
    "**Initialization**\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "\n",
    "The tf.global_variables_initializer() call returns an operation that will initialize all TensorFlow variables from the graph. You call the operation using a session to initialize all the variables as shown above. Using the tf.Variable class allows us to change the weights and bias, but an initial value needs to be chosen.\n",
    "\n",
    "\n",
    "Initializing the weights with random numbers from a normal distribution is good practice. Randomizing the weights helps the model from becoming stuck in the same place every time you train it. You'll learn more about this in the next lesson, when you study gradient descent.\n",
    "\n",
    "\n",
    "Similarly, choosing weights from a normal distribution prevents any one weight from overwhelming other weights. You'll use the tf.truncated_normal() function to generate random numbers from a normal distribution.\n",
    "\n",
    "    tf.truncated_normal()\n",
    "    n_features = 120\n",
    "    n_labels = 5\n",
    "    weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "\n",
    "\n",
    "The tf.truncated_normal() function returns a tensor with random values from a normal distribution whose magnitude is no more than 2 standard deviations from the mean.\n",
    "\n",
    "\n",
    "Since the weights are already helping prevent the model from getting stuck, you don't need to randomize the bias. Let's use the simplest solution, setting the bias to 0.\n",
    "\n",
    "\n",
    "    tf.zeros()\n",
    "    n_labels = 5\n",
    "    bias = tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "\n",
    "The tf.zeros() function returns a tensor with all zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Linear Classifier Quiz **\n",
    "\n",
    "<img src=\"images/mnist-012.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "You'll be classifying the handwritten numbers 0, 1, and 2 from the MNIST dataset using TensorFlow. The above is a small sample of the data you'll be training on. Notice how some of the 1s are written with a serif at the top and at different angles. The similarities and differences will play a part in shaping the weights of the model.\n",
    "\n",
    "<img src=\"images/weights-0-1-2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The images above are trained weights for each label (0, 1, and 2). The weights display the unique properties of each digit they have found. Complete this quiz to train your own weights using the MNIST dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "    weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "    return weights\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.zeros(n_labels))\n",
    "\n",
    "def linear(input, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    return tf.add(tf.matmul(input,w),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Loss: 8.99665641784668\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Sandbox Solution\n",
    "# Note: You can't run code in this tab\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        # In simple words we are just getting for first n numbers\n",
    "        # if n_labels is 3, we we will data for numbers 0, 1, 2\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels (i.e. first 3 numbers or classes)\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    ## Initilaize Variables : Key Step els we we get crazy error:\n",
    "    ## Attempting to use uninitialized value xxxxx...\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too much gone in above code, let's study key aspects ..\n",
    "\n",
    "** TensorFlow Softmax **\n",
    "\n",
    "The softmax function squashes it's inputs, typically called logits or logit scores, to be between 0 and 1 and also normalizes the outputs such that they all sum to 1. This means the output of the softmax function is equivalent to a categorical probability distribution. It's the perfect function to use as the output activation for a network predicting multiple classes.\n",
    "\n",
    "<img src=\"images/softmax-input-output.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "TensorFlow Softmax : We're using TensorFlow to build neural networks and, appropriately, there's a function for calculating softmax.\n",
    "\n",
    "x = tf.nn.softmax([2.0, 1.0, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax Output:  [ 0.65900117  0.24243298  0.09856589]\n",
      "Total of Softmax Outputs from Tensorflow, it shoud always add to 1 -- :  1.0000000447\n"
     ]
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = [2.0, 1.0, 0.1]\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "\n",
    "    softmax = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "\n",
    "o = run()\n",
    "\n",
    "print (\"Softmax Output: \", o)\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(o)):\n",
    "    sum += o[i]\n",
    "\n",
    "print (\"Total of Softmax Outputs from Tensorflow, it shoud always add to 1 -- : \", sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember Cross Entropy from NN Study??\n",
    "\n",
    "If not, search here : https://github.com/anshoomehra/Deep-Learning/blob/master/Neural%20Networks%20in%20Nut%20Shell.ipynb\n",
    "\n",
    "In summary: We take natura log of probablities to avoid multiplication errors with probabilities. Formula shown below, Something to keep in mind ** Higher the Probability results into Lower Cross Entropy i.e. higher values means higher ERROR Rate n vice-versa** \n",
    "\n",
    "<img src=\"images/cross-entropy-diagram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "To create a cross entropy function in TensorFlow, you'll need to use two new functions:\n",
    "\n",
    "    tf.reduce_sum()\n",
    "    tf.log()\n",
    "\n",
    "**Reduce Sum**\n",
    "    x = tf.reduce_sum([1, 2, 3, 4, 5])  # 15\n",
    "\n",
    "The tf.reduce_sum() function takes an array of numbers and sums them together.\n",
    "\n",
    "** Natural Log **\n",
    "    \n",
    "    x = tf.log(100.0)  # 4.60517\n",
    "\n",
    "This function does exactly what you would expect it to do. tf.log() takes the natural log of a number.\n",
    "\n",
    "So let's code example doing the same .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356675\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "# Multiply y [mentioned as one-hot] with natural_log(y_hat) [mentioned as softmax_data]\n",
    "# Sum all the output from above compution to get overall Cross-Entropy tell us overall error\n",
    "ce = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(ce, feed_dict={softmax:softmax_data,one_hot:one_hot_data})\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mini-batching ** \n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "Let's look at the MNIST dataset with weights and a bias to see if your machine can handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Train Feature Shape:  (55000, 784)\n",
      "Train Label Shape:  (55000, 10)\n",
      "Weights Shape:  (784, 10)\n",
      "Bias Shape:  (10,)\n",
      "Total Memory Needed in MB (float is 32 bits wisch is 4 bytes): 174.7114 MB\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "print (\"Train Feature Shape: \", train_features.shape)\n",
    "\n",
    "print (\"Train Label Shape: \", train_labels.shape)\n",
    "\n",
    "print (\"Weights Shape: \", weights.shape)\n",
    "\n",
    "print (\"Bias Shape: \", bias.shape)\n",
    "\n",
    "print (\"Total Memory Needed in MB (float is 32 bits wisch is 4 bytes):\",\\\n",
    "    (float((train_features.shape[0] * train_features.shape[1] * 4)) +\\\n",
    "    float(train_labels.shape[0] * train_labels.shape[1] * 4) +\\\n",
    "    float(str(weights.shape[0] * weights.shape[1] * 4)) +\\\n",
    "    float(str((bias.shape[0] * 4))))/1000000, \"MB\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total memory space required for the inputs, weights and bias is around 174 megabytes, which isn't that much memory. You could train this whole dataset on most CPUs and GPUs.\n",
    "\n",
    "But larger datasets that you'll use in the future measured in gigabytes or more. It's possible to purchase more memory, but it's expensive. A Titan X GPU with 12 GB of memory costs over $1,000.\n",
    "\n",
    "Instead, in order to run large models on your machine, you'll learn how to use mini-batching.\n",
    "\n",
    "Let's look at how you implement mini-batching in TensorFlow.\n",
    "\n",
    "**TensorFlow Mini-batching**\n",
    "In order to use mini-batching, you must first divide your data into batches.\n",
    "\n",
    "Unfortunately, it's sometimes impossible to divide the data into batches of exactly equal size. For example, imagine you'd like to create batches of 128 samples each from a dataset of 1000 samples. Since 128 does not evenly divide into 1000, you'd wind up with 7 batches of 128 samples, and 1 batch of 104 samples. (7*128 + 1*104 = 1000)\n",
    "\n",
    "In that case, the size of the batches would vary, so you need to take advantage of TensorFlow's tf.placeholder() function to receive the varying batch sizes.\n",
    "\n",
    "Continuing the example, if each sample had n_input = 784 features and n_classes = 10 possible labels, the dimensions for features would be [None, n_input] and labels would be [None, n_classes].\n",
    "\n",
    "    # Features and Labels\n",
    "    features = tf.placeholder(tf.float32, [None, n_input])\n",
    "    labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "What does None do here?\n",
    "\n",
    "The None dimension is a placeholder for the batch size. At runtime, TensorFlow will accept any batch size greater than 0.\n",
    "\n",
    "Going back to our earlier example, this setup allows you to feed features and labels into the model as either the batches of 128 samples or the single batch of 104 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    #print (len(features), len(labels))\n",
    "    samples = len(features) \n",
    "    output_batches = []\n",
    "    \n",
    "    for start_i in range(0, samples , batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        output_batches.append([features[start_i:end_i], labels[start_i:end_i]])\n",
    "        \n",
    "        #batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        #outout_batches.append(batch)\n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "#from quiz import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 Samples of features\n",
    "example_features = [\n",
    "    ['F11','F12','F13','F14'],\n",
    "    ['F21','F22','F23','F24'],\n",
    "    ['F31','F32','F33','F34'],\n",
    "    ['F41','F42','F43','F44']]\n",
    "# 4 Samples of labels\n",
    "example_labels = [\n",
    "    ['L11','L12'],\n",
    "    ['L21','L22'],\n",
    "    ['L31','L32'],\n",
    "    ['L41','L42']]\n",
    "\n",
    "# PPrint prints data structures like 2d arrays, so they are easier to read\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mini-batching to feed batches of MNIST features and labels into a linear model.\n",
    "\n",
    "Set the batch size and run the optimizer over all the batches with the batches function. The recommended batch size is 128. If you have memory restrictions, feel free to make it smaller.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    outout_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        outout_batches.append(batch)\n",
    "        \n",
    "    return outout_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing Batch : 1\n",
      "Processing Batch : 5\n",
      "Processing Batch : 9\n",
      "Processing Batch : 13\n",
      "Processing Batch : 17\n",
      "Processing Batch : 21\n",
      "Processing Batch : 25\n",
      "Processing Batch : 29\n",
      "Processing Batch : 33\n",
      "Processing Batch : 37\n",
      "Processing Batch : 41\n",
      "Processing Batch : 45\n",
      "Processing Batch : 49\n",
      "Processing Batch : 53\n",
      "Processing Batch : 57\n",
      "Processing Batch : 61\n",
      "Processing Batch : 65\n",
      "Processing Batch : 69\n",
      "Processing Batch : 73\n",
      "Processing Batch : 77\n",
      "Processing Batch : 81\n",
      "Processing Batch : 85\n",
      "Processing Batch : 89\n",
      "Processing Batch : 93\n",
      "Processing Batch : 97\n",
      "Processing Batch : 101\n",
      "Processing Batch : 105\n",
      "Processing Batch : 109\n",
      "Processing Batch : 113\n",
      "Processing Batch : 117\n",
      "Processing Batch : 121\n",
      "Processing Batch : 125\n",
      "Processing Batch : 129\n",
      "Processing Batch : 133\n",
      "Processing Batch : 137\n",
      "Processing Batch : 141\n",
      "Processing Batch : 145\n",
      "Processing Batch : 149\n",
      "Processing Batch : 153\n",
      "Processing Batch : 157\n",
      "Processing Batch : 161\n",
      "Processing Batch : 165\n",
      "Processing Batch : 169\n",
      "Processing Batch : 173\n",
      "Processing Batch : 177\n",
      "Processing Batch : 181\n",
      "Processing Batch : 185\n",
      "Processing Batch : 189\n",
      "Processing Batch : 193\n",
      "Processing Batch : 197\n",
      "Processing Batch : 201\n",
      "Processing Batch : 205\n",
      "Processing Batch : 209\n",
      "Processing Batch : 213\n",
      "Processing Batch : 217\n",
      "Processing Batch : 221\n",
      "Processing Batch : 225\n",
      "Processing Batch : 229\n",
      "Processing Batch : 233\n",
      "Processing Batch : 237\n",
      "Processing Batch : 241\n",
      "Processing Batch : 245\n",
      "Processing Batch : 249\n",
      "Processing Batch : 253\n",
      "Processing Batch : 257\n",
      "Processing Batch : 261\n",
      "Processing Batch : 265\n",
      "Processing Batch : 269\n",
      "Processing Batch : 273\n",
      "Processing Batch : 277\n",
      "Processing Batch : 281\n",
      "Processing Batch : 285\n",
      "Processing Batch : 289\n",
      "Processing Batch : 293\n",
      "Processing Batch : 297\n",
      "Processing Batch : 301\n",
      "Processing Batch : 305\n",
      "Processing Batch : 309\n",
      "Processing Batch : 313\n",
      "Processing Batch : 317\n",
      "Processing Batch : 321\n",
      "Processing Batch : 325\n",
      "Processing Batch : 329\n",
      "Processing Batch : 333\n",
      "Processing Batch : 337\n",
      "Processing Batch : 341\n",
      "Processing Batch : 345\n",
      "Processing Batch : 349\n",
      "Processing Batch : 353\n",
      "Processing Batch : 357\n",
      "Processing Batch : 361\n",
      "Processing Batch : 365\n",
      "Processing Batch : 369\n",
      "Processing Batch : 373\n",
      "Processing Batch : 377\n",
      "Processing Batch : 381\n",
      "Processing Batch : 385\n",
      "Processing Batch : 389\n",
      "Processing Batch : 393\n",
      "Processing Batch : 397\n",
      "Processing Batch : 401\n",
      "Processing Batch : 405\n",
      "Processing Batch : 409\n",
      "Processing Batch : 413\n",
      "Processing Batch : 417\n",
      "Processing Batch : 421\n",
      "Processing Batch : 425\n",
      "Processing Batch : 429\n",
      "Test Accuracy: 0.09719999879598618\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from helper import batches\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# TODO: Set batch size\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'You must set the batch size'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    # TODO: Train optimizer on all batches\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        if i % 4 ==0:\n",
    "            print (\"Processing Batch :\", i+1)\n",
    "            \n",
    "        i += 1\n",
    "        sess.run(optimizer, feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Epochs **\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. This section will cover epochs in TensorFlow and how to choose the right number of epochs.\n",
    "\n",
    "The following TensorFlow code trains a model using 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/ud730/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/ud730/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0    - Cost: 11.6     Valid Accuracy: 0.101\n",
      "Epoch: 1    - Cost: 10.4     Valid Accuracy: 0.117\n",
      "Epoch: 2    - Cost: 9.64     Valid Accuracy: 0.134\n",
      "Epoch: 3    - Cost: 9.03     Valid Accuracy: 0.147\n",
      "Epoch: 4    - Cost: 8.52     Valid Accuracy: 0.164\n",
      "Epoch: 5    - Cost: 8.09     Valid Accuracy: 0.178\n",
      "Epoch: 6    - Cost: 7.71     Valid Accuracy: 0.191\n",
      "Epoch: 7    - Cost: 7.36     Valid Accuracy: 0.21 \n",
      "Epoch: 8    - Cost: 7.04     Valid Accuracy: 0.227\n",
      "Epoch: 9    - Cost: 6.74     Valid Accuracy: 0.244\n",
      "Epoch: 10   - Cost: 6.46     Valid Accuracy: 0.26 \n",
      "Epoch: 11   - Cost: 6.19     Valid Accuracy: 0.277\n",
      "Epoch: 12   - Cost: 5.94     Valid Accuracy: 0.292\n",
      "Epoch: 13   - Cost: 5.7      Valid Accuracy: 0.307\n",
      "Epoch: 14   - Cost: 5.48     Valid Accuracy: 0.323\n",
      "Epoch: 15   - Cost: 5.27     Valid Accuracy: 0.34 \n",
      "Epoch: 16   - Cost: 5.07     Valid Accuracy: 0.355\n",
      "Epoch: 17   - Cost: 4.88     Valid Accuracy: 0.368\n",
      "Epoch: 18   - Cost: 4.7      Valid Accuracy: 0.379\n",
      "Epoch: 19   - Cost: 4.54     Valid Accuracy: 0.39 \n",
      "Epoch: 20   - Cost: 4.38     Valid Accuracy: 0.404\n",
      "Epoch: 21   - Cost: 4.24     Valid Accuracy: 0.418\n",
      "Epoch: 22   - Cost: 4.1      Valid Accuracy: 0.429\n",
      "Epoch: 23   - Cost: 3.98     Valid Accuracy: 0.443\n",
      "Epoch: 24   - Cost: 3.86     Valid Accuracy: 0.454\n",
      "Epoch: 25   - Cost: 3.75     Valid Accuracy: 0.461\n",
      "Epoch: 26   - Cost: 3.65     Valid Accuracy: 0.471\n",
      "Epoch: 27   - Cost: 3.55     Valid Accuracy: 0.481\n",
      "Epoch: 28   - Cost: 3.46     Valid Accuracy: 0.488\n",
      "Epoch: 29   - Cost: 3.38     Valid Accuracy: 0.498\n",
      "Epoch: 30   - Cost: 3.3      Valid Accuracy: 0.507\n",
      "Epoch: 31   - Cost: 3.23     Valid Accuracy: 0.517\n",
      "Epoch: 32   - Cost: 3.16     Valid Accuracy: 0.523\n",
      "Epoch: 33   - Cost: 3.09     Valid Accuracy: 0.528\n",
      "Epoch: 34   - Cost: 3.03     Valid Accuracy: 0.536\n",
      "Epoch: 35   - Cost: 2.98     Valid Accuracy: 0.543\n",
      "Epoch: 36   - Cost: 2.92     Valid Accuracy: 0.547\n",
      "Epoch: 37   - Cost: 2.87     Valid Accuracy: 0.553\n",
      "Epoch: 38   - Cost: 2.82     Valid Accuracy: 0.559\n",
      "Epoch: 39   - Cost: 2.78     Valid Accuracy: 0.566\n",
      "Epoch: 40   - Cost: 2.74     Valid Accuracy: 0.571\n",
      "Epoch: 41   - Cost: 2.7      Valid Accuracy: 0.576\n",
      "Epoch: 42   - Cost: 2.66     Valid Accuracy: 0.58 \n",
      "Epoch: 43   - Cost: 2.62     Valid Accuracy: 0.584\n",
      "Epoch: 44   - Cost: 2.58     Valid Accuracy: 0.589\n",
      "Epoch: 45   - Cost: 2.55     Valid Accuracy: 0.594\n",
      "Epoch: 46   - Cost: 2.52     Valid Accuracy: 0.597\n",
      "Epoch: 47   - Cost: 2.49     Valid Accuracy: 0.601\n",
      "Epoch: 48   - Cost: 2.46     Valid Accuracy: 0.605\n",
      "Epoch: 49   - Cost: 2.43     Valid Accuracy: 0.609\n",
      "Epoch: 50   - Cost: 2.4      Valid Accuracy: 0.615\n",
      "Epoch: 51   - Cost: 2.37     Valid Accuracy: 0.618\n",
      "Epoch: 52   - Cost: 2.35     Valid Accuracy: 0.622\n",
      "Epoch: 53   - Cost: 2.32     Valid Accuracy: 0.624\n",
      "Epoch: 54   - Cost: 2.3      Valid Accuracy: 0.629\n",
      "Epoch: 55   - Cost: 2.28     Valid Accuracy: 0.631\n",
      "Epoch: 56   - Cost: 2.25     Valid Accuracy: 0.635\n",
      "Epoch: 57   - Cost: 2.23     Valid Accuracy: 0.637\n",
      "Epoch: 58   - Cost: 2.21     Valid Accuracy: 0.639\n",
      "Epoch: 59   - Cost: 2.19     Valid Accuracy: 0.642\n",
      "Epoch: 60   - Cost: 2.17     Valid Accuracy: 0.645\n",
      "Epoch: 61   - Cost: 2.15     Valid Accuracy: 0.648\n",
      "Epoch: 62   - Cost: 2.14     Valid Accuracy: 0.651\n",
      "Epoch: 63   - Cost: 2.12     Valid Accuracy: 0.653\n",
      "Epoch: 64   - Cost: 2.1      Valid Accuracy: 0.655\n",
      "Epoch: 65   - Cost: 2.08     Valid Accuracy: 0.657\n",
      "Epoch: 66   - Cost: 2.07     Valid Accuracy: 0.659\n",
      "Epoch: 67   - Cost: 2.05     Valid Accuracy: 0.661\n",
      "Epoch: 68   - Cost: 2.04     Valid Accuracy: 0.662\n",
      "Epoch: 69   - Cost: 2.02     Valid Accuracy: 0.664\n",
      "Epoch: 70   - Cost: 2.01     Valid Accuracy: 0.667\n",
      "Epoch: 71   - Cost: 1.99     Valid Accuracy: 0.67 \n",
      "Epoch: 72   - Cost: 1.98     Valid Accuracy: 0.673\n",
      "Epoch: 73   - Cost: 1.96     Valid Accuracy: 0.674\n",
      "Epoch: 74   - Cost: 1.95     Valid Accuracy: 0.676\n",
      "Epoch: 75   - Cost: 1.94     Valid Accuracy: 0.678\n",
      "Epoch: 76   - Cost: 1.92     Valid Accuracy: 0.681\n",
      "Epoch: 77   - Cost: 1.91     Valid Accuracy: 0.683\n",
      "Epoch: 78   - Cost: 1.9      Valid Accuracy: 0.685\n",
      "Epoch: 79   - Cost: 1.89     Valid Accuracy: 0.687\n",
      "Epoch: 80   - Cost: 1.88     Valid Accuracy: 0.689\n",
      "Epoch: 81   - Cost: 1.87     Valid Accuracy: 0.691\n",
      "Epoch: 82   - Cost: 1.85     Valid Accuracy: 0.692\n",
      "Epoch: 83   - Cost: 1.84     Valid Accuracy: 0.694\n",
      "Epoch: 84   - Cost: 1.83     Valid Accuracy: 0.695\n",
      "Epoch: 85   - Cost: 1.82     Valid Accuracy: 0.697\n",
      "Epoch: 86   - Cost: 1.81     Valid Accuracy: 0.698\n",
      "Epoch: 87   - Cost: 1.8      Valid Accuracy: 0.7  \n",
      "Epoch: 88   - Cost: 1.79     Valid Accuracy: 0.701\n",
      "Epoch: 89   - Cost: 1.78     Valid Accuracy: 0.701\n",
      "Epoch: 90   - Cost: 1.77     Valid Accuracy: 0.703\n",
      "Epoch: 91   - Cost: 1.76     Valid Accuracy: 0.704\n",
      "Epoch: 92   - Cost: 1.75     Valid Accuracy: 0.706\n",
      "Epoch: 93   - Cost: 1.75     Valid Accuracy: 0.708\n",
      "Epoch: 94   - Cost: 1.74     Valid Accuracy: 0.709\n",
      "Epoch: 95   - Cost: 1.73     Valid Accuracy: 0.71 \n",
      "Epoch: 96   - Cost: 1.72     Valid Accuracy: 0.712\n",
      "Epoch: 97   - Cost: 1.71     Valid Accuracy: 0.714\n",
      "Epoch: 98   - Cost: 1.7      Valid Accuracy: 0.715\n",
      "Epoch: 99   - Cost: 1.69     Valid Accuracy: 0.717\n",
      "Test Accuracy: 0.7217000126838684\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#from helper import batches  # Helper function created in Mini-batching section\n",
    "\n",
    "\n",
    "def print_epoch_stats(epoch_i, sess, last_features, last_labels):\n",
    "    \"\"\"\n",
    "    Print cost and validation accuracy of an epoch\n",
    "    \"\"\"\n",
    "    current_cost = sess.run(\n",
    "        cost,\n",
    "        feed_dict={features: last_features, labels: last_labels})\n",
    "    valid_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: valid_features, labels: valid_labels})\n",
    "    print('Epoch: {:<4} - Cost: {:<8.3} Valid Accuracy: {:<5.3}'.format(\n",
    "        epoch_i,\n",
    "        current_cost,\n",
    "        valid_accuracy))\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "# The features are already scaled and the data is shuffled\n",
    "train_features = mnist.train.images\n",
    "valid_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "valid_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "learn_rate = 0.001\n",
    "\n",
    "train_batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch_i in range(epochs):\n",
    "\n",
    "        # Loop over all batches\n",
    "        for batch_features, batch_labels in train_batches:\n",
    "            train_feed_dict = {\n",
    "                features: batch_features,\n",
    "                labels: batch_labels,\n",
    "                learning_rate: learn_rate}\n",
    "            sess.run(optimizer, feed_dict=train_feed_dict)\n",
    "\n",
    "        # Print cost and validation accuracy of an epoch\n",
    "        print_epoch_stats(epoch_i, sess, batch_features, batch_labels)\n",
    "\n",
    "    # Calculate accuracy for test dataset\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: test_features, labels: test_labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Project in Tensorflow\n",
    "\n",
    "https://github.com/anshoomehra/udacity-deep-learning/tree/master/intro-to-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Neural Networks with Tensorflow\n",
    "\n",
    "So we saw good amount of projects to do single layer networks above. Below we will use start dealing with more complexity adding more layers to out network, afterall that is what defines Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ReLUs **\n",
    "\n",
    "We studied ReLU activation function during NN class. We will use the same here as activation function for hidden layer.\n",
    "\n",
    "ReLUs are like switches, by turning-off all negative weights, this helps make our network handle non-linearity, subseuqnetly  handling more complex functions.\n",
    "\n",
    "So how do we achieve this in tensorflow ..\n",
    "\n",
    "**TensorFlow ReLUs**\n",
    "TensorFlow provides the ReLU function as tf.nn.relu(), as shown below.\n",
    "\n",
    "    # Hidden Layer with ReLU activation function\n",
    "    hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "    output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)\n",
    "\n",
    "\n",
    "The above code applies the tf.nn.relu() function to the hidden_layer, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n",
    "<img src=\"images/relu-network.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "We've seen the linear function tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer']) before, also known as xw + b. Combining linear functions together using a ReLU will give you a two layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.11000013   8.44000053]\n",
      " [  0.           0.        ]\n",
      " [ 24.01000214  38.23999786]]\n"
     ]
    }
   ],
   "source": [
    "### Simple Example of using ReLU Activation\n",
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network in TensorFlow\n",
    "\n",
    "You've seen how to build a logistic classifier using TensorFlow. Now you're going to see how to use the logistic classifier to build a deep neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 47.498840332\n",
      "Epoch: 0002 cost= 28.323375702\n",
      "Epoch: 0003 cost= 22.000427246\n",
      "Epoch: 0004 cost= 14.770172119\n",
      "Epoch: 0005 cost= 13.637517929\n",
      "Epoch: 0006 cost= 19.591865540\n",
      "Epoch: 0007 cost= 13.236281395\n",
      "Epoch: 0008 cost= 12.072938919\n",
      "Epoch: 0009 cost= 9.566403389\n",
      "Epoch: 0010 cost= 8.450783730\n",
      "Epoch: 0011 cost= 6.087422371\n",
      "Epoch: 0012 cost= 6.903230190\n",
      "Epoch: 0013 cost= 7.663883686\n",
      "Epoch: 0014 cost= 8.361234665\n",
      "Epoch: 0015 cost= 5.720129967\n",
      "Epoch: 0016 cost= 5.607900620\n",
      "Epoch: 0017 cost= 9.265755653\n",
      "Epoch: 0018 cost= 3.941402674\n",
      "Epoch: 0019 cost= 6.534650326\n",
      "Epoch: 0020 cost= 5.376499176\n",
      "Optimization Finished!\n",
      "Accuracy: 0.839844\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True, reshape=False)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(c))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # Decrease test_size if you don't have enough memory\n",
    "    test_size = 256\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:test_size], y: mnist.test.labels[:test_size]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save and Restore TensorFlow Models**\n",
    "Training a model can take hours. But once you close your TensorFlow session, you lose all the trained weights and biases. If you were to reuse the model in the future, you would have to train it all over again!\n",
    "\n",
    "Fortunately, TensorFlow gives you the ability to save your progress using a class called tf.train.Saver. This class provides the functionality to save any tf.Variable to your file system.\n",
    "\n",
    "**Saving Variables**\n",
    "Let's start with a simple example of saving weights and bias Tensors. For the first example you'll just save two variables. Later examples will save all the weights in a practical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-0.98569649  0.68323284 -1.91659582]\n",
      " [ 0.61796433  1.05859601  0.39688444]]\n",
      "Bias:\n",
      "[ 0.62646139 -0.83633977 -0.67695129]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# The file path to save the data\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "w = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "b = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all the Variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weights:')\n",
    "    print(sess.run(w))\n",
    "    print('Bias:')\n",
    "    print(sess.run(b))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensors weights and bias are set to random values using the tf.truncated_normal() function. The values are then saved to the save_file location, \"model.ckpt\", using the tf.train.Saver.save() function. (The \".ckpt\" extension stands for \"checkpoint\".)\n",
    "\n",
    "If you're using TensorFlow 0.11.0RC1 or newer, a file called \"model.ckpt.meta\" will also be created. This file contains the TensorFlow graph.\n",
    "\n",
    "**Loading Variables**\n",
    "Now that the Tensor Variables are saved, let's load them back into a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Weight:\n",
      "[[ 0.15486424 -0.3900055   1.08488607]\n",
      " [-1.51948488  0.14477894 -0.08510909]]\n",
      "Bias:\n",
      "[ 0.68925333  0.22952777  0.39401624]\n"
     ]
    }
   ],
   "source": [
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "w = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "b = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Class used to save and/or restore Tensor Variables\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    # Show the values of weights and bias\n",
    "    print('Weight:')\n",
    "    print(sess.run(w))\n",
    "    print('Bias:')\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice you still need to create the weights and bias Tensors in Python. The tf.train.Saver.restore() function loads the saved data into weights and bias.\n",
    "\n",
    "Since tf.train.Saver.restore() sets all the TensorFlow Variables, you don't need to call tf.global_variables_initializer().\n",
    "\n",
    "**Save a Trained Model**\n",
    "Let's see how to train a model and save its weights.\n",
    "\n",
    "First start with a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Remove previous Tensors and Operations\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Weights & bias\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "# Logits - xW + b\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(\\\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\\\n",
    "    .minimize(cost)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0   - Validation Accuracy: 0.1242000013589859\n",
      "Epoch 10  - Validation Accuracy: 0.24860000610351562\n",
      "Epoch 20  - Validation Accuracy: 0.37940001487731934\n",
      "Epoch 30  - Validation Accuracy: 0.48339998722076416\n",
      "Epoch 40  - Validation Accuracy: 0.5432000160217285\n",
      "Epoch 50  - Validation Accuracy: 0.5896000266075134\n",
      "Epoch 60  - Validation Accuracy: 0.6223999857902527\n",
      "Epoch 70  - Validation Accuracy: 0.6484000086784363\n",
      "Epoch 80  - Validation Accuracy: 0.6732000112533569\n",
      "Epoch 90  - Validation Accuracy: 0.6941999793052673\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "save_file = './train_model.ckpt'\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(n_epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(\n",
    "                optimizer,\n",
    "                feed_dict={features: batch_features, labels: batch_labels})\n",
    "\n",
    "        # Print status for every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            valid_accuracy = sess.run(\n",
    "                accuracy,\n",
    "                feed_dict={\n",
    "                    features: mnist.validation.images,\n",
    "                    labels: mnist.validation.labels})\n",
    "            print('Epoch {:<3} - Validation Accuracy: {}'.format(\n",
    "                epoch,\n",
    "                valid_accuracy))\n",
    "\n",
    "    # Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load a Trained Model**\n",
    "Let's load the weights and bias from memory, then check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n",
      "Test Accuracy: 0.7236999869346619\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={features: mnist.test.images, labels: mnist.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Weights and Biases into a New Model**\n",
    "\n",
    "Sometimes you might want to adjust, or \"finetune\" a model that you have already trained and saved.\n",
    "\n",
    "However, loading saved Variables directly into a modified model can generate errors. Let's go over how to avoid these problems.\n",
    "\n",
    "**Naming Error**\n",
    "\n",
    "TensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name <Type>, and then give the name <Type>_<number> for the subsequent nodes. Let's see how this can affect loading a model with a different order of weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: Variable:0\n",
      "Save Bias: Variable_1:0\n",
      "Load Weights: Variable_1:0\n",
      "Load Bias: Variable:0\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Assign requires shapes of both tensors to match. lhs shape= [2,3] rhs shape= [3]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, save/RestoreV2_1)]]\n\nCaused by op 'save/Assign_1', defined at:\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-138-32cce566247b>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [2,3] rhs shape= [3]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, save/RestoreV2_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [2,3] rhs shape= [3]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, save/RestoreV2_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-32cce566247b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Load the weights and bias - ERROR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1457\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Assign requires shapes of both tensors to match. lhs shape= [2,3] rhs shape= [3]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, save/RestoreV2_1)]]\n\nCaused by op 'save/Assign_1', defined at:\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-138-32cce566247b>\", line 29, in <module>\n    saver = tf.train.Saver()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 419, in _AddRestoreOps\n    assign_ops.append(saveable.restore(tensors, shapes))\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 155, in restore\n    self.op.get_shape().is_fully_defined())\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 270, in assign\n    validate_shape=validate_shape)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\", line 47, in assign\n    use_locking=use_locking, name=name)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/anmehra/Desktop/Anconda_Py2.7/anaconda/envs/dlnd/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [2,3] rhs shape= [3]\n\t [[Node: save/Assign_1 = Assign[T=DT_FLOAT, _class=[\"loc:@Variable_1\"], use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1, save/RestoreV2_1)]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - ERROR\n",
    "    saver.restore(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the name properties for weights and bias are different than when you saved the model. This is why the code produces the \"Assign requires shapes of both tensors to match\" error. The code saver.restore(sess, save_file) is trying to load weight data into bias and bias data into weights.\n",
    "\n",
    "Instead of letting TensorFlow set the name property, let's set it manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Weights: weights_0:0\n",
      "Save Bias: bias_0:0\n",
      "Load Weights: weights_0:0\n",
      "Load Bias: bias_0:0\n",
      "INFO:tensorflow:Restoring parameters from model.ckpt\n",
      "Loaded Weights and Bias successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "save_file = 'model.ckpt'\n",
    "\n",
    "# Two Tensor Variables: weights and bias\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]), name='weights_0')\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Save Weights: {}'.format(weights.name))\n",
    "print('Save Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.save(sess, save_file)\n",
    "\n",
    "# Remove the previous weights and bias\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Two Variables: weights and bias\n",
    "bias = tf.Variable(tf.truncated_normal([3]), name='bias_0')\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]) ,name='weights_0')\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Print the name of Weights and Bias\n",
    "print('Load Weights: {}'.format(weights.name))\n",
    "print('Load Bias: {}'.format(bias.name))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Load the weights and bias - No Error\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "print('Loaded Weights and Bias successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Dropout\n",
    "\n",
    "<img src=\"images/dropout-node.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (artificial neurons) from the network, along with all of those units' incoming and outgoing connections. Figure 1 illustrates how dropout works.\n",
    "\n",
    "TensorFlow provides the tf.nn.dropout() function, which you can use to implement dropout.\n",
    "\n",
    "Let's look at an example of how to use tf.nn.dropout().\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "    hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "    logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "The code above illustrates how to apply dropout to a neural network.\n",
    "\n",
    "The tf.nn.dropout() function takes in two parameters:\n",
    "\n",
    "1. hidden_layer: the tensor to which you would like to apply dropout\n",
    "2. keep_prob: the probability of keeping (i.e. not dropping) any given unit\n",
    "\n",
    "\n",
    "keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.\n",
    "\n",
    "During training, a good starting value for keep_prob is 0.5.\n",
    "\n",
    "During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's Code!. **\n",
    "\n",
    "** Note: Output will be different every time the code is run. This is caused by dropout randomizing the units it drops. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.10000002   6.60000038]\n",
      " [  0.30800003   0.7700001 ]\n",
      " [ 43.30000305  48.15999985]]\n"
     ]
    }
   ],
   "source": [
    "# Quiz Solution\n",
    "# Note: You can't run code in this tab\n",
    "import tensorflow as tf\n",
    "\n",
    "hidden_layer_weights = [\n",
    "    [0.1, 0.2, 0.4],\n",
    "    [0.4, 0.6, 0.6],\n",
    "    [0.5, 0.9, 0.1],\n",
    "    [0.8, 0.2, 0.8]]\n",
    "out_weights = [\n",
    "    [0.1, 0.6],\n",
    "    [0.2, 0.1],\n",
    "    [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [\n",
    "    tf.Variable(hidden_layer_weights),\n",
    "    tf.Variable(out_weights)]\n",
    "biases = [\n",
    "    tf.Variable(tf.zeros(3)),\n",
    "    tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[0.0, 2.0, 3.0, 4.0], [0.1, 0.2, 0.3, 0.4], [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model with Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print logits from a session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits, feed_dict={keep_prob: 0.5}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
