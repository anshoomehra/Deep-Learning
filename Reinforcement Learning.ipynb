{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intent to use this jupyter for myself, quick refresher / class-notes for course material studied for  Neural Networks from Udacity [ & not to reproduce any material as such ..]\n",
    "\n",
    "This guide DOES NOT go in depth explaining fundamentals, & may appear be in arbitraty sequence to follow, as it is aligned to class labs. One should go through proper course material prior using this guide. I highly recommend Udacity nanodegree course(s) as right medium to learn this subject.\n",
    "\n",
    "Content Credit: Udacity\n",
    "\n",
    "https://github.com/anshoomehra/udacity-deep-learning/tree/master/reinforcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is RL?\n",
    "Reinforcement learning (RL) is an area of machine learning inspired by behaviourist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. \n",
    "\n",
    "\n",
    "# RL Application Examples\n",
    "\n",
    "Read about [TD-Gammon](https://courses.cs.washington.edu/courses/cse590hk/01sp/Readings/tesauro95cacm.pdf), one of the first successful applications of neural networks to reinforcement learning.\n",
    "\n",
    "Read about [AlphaGo Zero](https://deepmind.com/blog/alphago-zero-learning-scratch/), the state-of-the-art computer program that defeats professional human Go players.\n",
    "\n",
    "Learn about how reinforcement learning (RL) is used to play [Atari games](https://deepmind.com/research/dqn/).\n",
    "\n",
    "Read about OpenAI's bot that beat the worldâ€™s top players of [Dota 2](http://www.dota2.com/play/).\n",
    "\n",
    "Read about [research](https://classroom.udacity.com/nanodegrees/nd101/parts/7d0218b1-1a81-4d49-95f7-14b015020851/modules/38c16ab9-91e8-44ce-a0ce-ae1b9ba12146/lessons/2942b8b9-76c1-451d-879c-3d31d3ac00c8/concepts/(https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/) used to teach humanoid bodies to walk.\n",
    "\n",
    "Learn about RL for [self-driving cars](http://selfdrivingcars.mit.edu/).\n",
    "\n",
    "To see an example of RL applied to finance, check out this [final project](https://github.com/ucaiado/QLearning_Trading) from a student who graduated from the \n",
    "Machine Learning Engineer Nanodegree.\n",
    "\n",
    "Learn about RL for [telecommunication](https://papers.nips.cc/paper/1740-low-power-wireless-communication-via-reinforcement-learning.pdf).\n",
    "\n",
    "Read this paper that introduces RL for [inventory management](https://goo.gl/e3gaM2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [OpenAI Gym](https://github.com/openai/gym) : OpenAI Gym is a toolkit for developing and comparing reinforcement learning algorithms\n",
    "\n",
    "You're encouraged to take the time to check out the [leaderboard](https://github.com/openai/gym/wiki/Leaderboard), which contains the best solutions to each task.\n",
    "\n",
    "Check out this [blog post](https://blog.openai.com/openai-gym-beta/) to read more about how OpenAI Gym is used to accelerate reinforcement learning (RL) research.\n",
    "\n",
    "**Installation Instructions**\n",
    "If you'd like to install OpenAI Gym on your machine, you are encouraged to perform a minimal install:\n",
    "\n",
    "    git clone https://github.com/openai/gym.git\n",
    "    cd gym\n",
    "    pip install -e .\n",
    "\n",
    "\n",
    "Once you have installed OpenAI Gym, obtain the code for the classic control tasks (such as 'CartPole-v0'):\n",
    "\n",
    "    pip install -e '.[classic_control]'\n",
    "\n",
    "\n",
    "Finally, check your installation by running the simple [random agent](https://github.com/openai/gym/blob/master/examples/agents/random_agent.py) provided in the examples directory.\n",
    "\n",
    "    cd examples/agents\n",
    "    python random_agent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book : https://s3-us-west-1.amazonaws.com/udacity-dlnfd/suttonbookdraft2018jan1.pdf\n",
    "\n",
    "Github for Book Labs: https://github.com/ShangtongZhang/reinforcement-learning-an-introduction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning - from this point on, I am attempting to mix my perspective with learning-guide notes, deadly mixture, if my undertsanding is wrong, it may rather confuse one or set up wrong founation .. \n",
    "\n",
    "** [I will eventually learn my mistakes via Human-RL :-) and come back to course correct any such mistakes .. until then if you are reading, use your best judgement along with reading ] **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having this off my chest, let's get started :-)\n",
    "\n",
    "First Q which bothered me more than anything else was:\n",
    "\n",
    "Q - How to differentate simple NN, CovNets, RNNs etc from notion of Reinforcement Learning??\n",
    "\n",
    "In my humble opinion, what we are doing with various modeling techniques is train, test, deploy -- meaning once we deploy, we are out of learning mode. I will stop here and have you read this again, ' once we deploy, we are out of leaning mode ' -- I knew this all along but this statement didn't struck met yet in the perspective I should have.\n",
    "\n",
    "Let me elaborate, in past I have been building say classification/regression models, once accuracy score meet expections we would take them ahead and deploy for consumption in real-world, we will further keep track of metrics on how we are doing in real world. If there are failures, analyze them, be fault of training, training data, or completely new classes we should train again. Take these feedback back to development-pipeline and come back with new model ...\n",
    "\n",
    "What did we miss here?? That there is disconnect between when model is consumed and trained, that is even though we have feedback real-time during consumtion phase, we have no way to retrain the model not repeat failures or get better at them ... I think this where term 'Reinforcement Learning' come in play, where systems are designed in way where learn by themselves, meaning they are learning, getting real-time feedback in the form of incentives, if they do good job, if not, they have to a better job! Sounds very interesting & intriguing right?? Let's work it out further understanding this in depth and how to implement the same ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Intuition **\n",
    "\n",
    "**AGENT** - Let's say process which has to learn is called AGENT. Agent can be anything like a Robot, Self-Driving or just a Software Bot like AI Game (like AlphaGo)..\n",
    "\n",
    "**ENVIRONMENT** - Let's say Agent is trying to learn some environment, it could be anything like Game AlphaGo, so Game is Environment - Agent as AI Bot, Car driving to learn on Road so Road is Environment - Agent as Car ..\n",
    "\n",
    "**How will AGENT learn ENVIRONMENT**\n",
    "1. ENVIRONMENT will give OBSERVATION (of the State)\n",
    "2. AGENT will make some ACTION\n",
    "3. ENVIRONMENT will issue REWARD if ACTION was inline with expecations.\n",
    "\n",
    "**GOAL** is to earn Maximum Cumulative REWARD, and in these process iterations, AGENT will eventually learn better n better with time .. Small depiction below.\n",
    "\n",
    "![RL Simple View](images/rl1.png)\n",
    "\n",
    "\n",
    "To be precise, let's start calling Observation as State and have this process be drawn as time-series, i.e. seuqnece of steps like State, Action, Reward, State, Action, Reward ..and to depict time-series, subscript notion will mark actions in time state 0, 1, 2 etc ..\n",
    "\n",
    "![RL Simple View](images/rl2.png)\n",
    "\n",
    "We made lot's of assumptions, that Env has well define State and over-simplified few things like Reward, Rewards are not all same & given only if Agent follows the Rules, which means theer are Rules defined by Environment. I think I should stop here, it would be hopefully implicit beyond this to go level deep and tie all this together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASKS - THE PROCESS OF LEARNING **\n",
    "\n",
    "** Episodic Tasks ** Interactions which ends at time step T. In essence, the rewards will\n",
    "be cumulative accumulation until this step T. Example, if Agent is lesrning to play Atari video-game\n",
    "after the game finished and based on winning or loosing cumilative rewards will be decided. Once the Episode is finished, and in next iteration, Agent will start from scractch, as if it's re-born but this time with Prior Knowledge. As time pass by, ideally Agent should be able to learn better and at some point will be able to define strategies which can target to achieve maximum rewards (in simple words using this example will master techniques to achieve maximum score) \n",
    "\n",
    "![Episodic Tasks](images/rl3.png)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "** Continuing Tasks ** These tasks differ on the grounds that they have no well defined ending, and these Agent will continue to live forever, relying on real-time feedback. These are bit complex in nature, good example would be stock trading agent as shown below..\n",
    "\n",
    "![Episodic Tasks](images/rl4.png)\n",
    "\n",
    "\n",
    "Remember:\n",
    "\n",
    "- A task is an instance of the reinforcement learning (RL) problem.\n",
    "- Continuing tasks are tasks that continue forever, without end.\n",
    "- Episodic tasks are tasks with a well-defined starting and ending point.\n",
    " - In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.\n",
    " - Episodic tasks come to an end whenever the agent reaches a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards Hypothesis\n",
    "\n",
    "Just as quick refresher, goal of Agent is to maxime cumulative reward.\n",
    "\n",
    "![Reward Hypothesis](images/rl5.png)\n",
    "\n",
    "** So how do we specify reward ?? **\n",
    "\n",
    "Let's take intriguing example of Google Deepmind's effort to design humoid learning to walk. Humanoid has joints and action to learn to walk is basically velocities on joints in harmorny leading to walking posture without falling. This is defined as Episodic task, with terminal state as humanoid fall, any time humaoid fall will terminate the Episode.\n",
    "\n",
    "So what are the States (as-in inputs) + Actions humanoid (Agent) has in this example --\n",
    "\n",
    "![Reward Hypothesis](images/rl6.png)\n",
    "![Reward Hypothesis](images/rl7.png)\n",
    "\n",
    "We have good handle on States and Actions, so What are Rewards??\n",
    "\n",
    "From Google Deepmind's researchm below is the simplifies rewards equation.\n",
    "\n",
    "![Reward Hypothesis](images/rl8.png)\n",
    "\n",
    "Each part of this equations tells us something, let's look at it more closely ..\n",
    "\n",
    "1. We have reward for humanoid to walk faster, but to a certain limit, this limit is defined as Vmax. First part of equation demonstrate the same.\n",
    "\n",
    "    ![Reward Hypothesis](images/rl9.png)\n",
    "<br>\n",
    "2. Above reward is proportional to forces applied to the joints, more force applied penalize the action, this helps achieve smooth walking motion and avoid erradic walking posture.\n",
    "\n",
    "    ![Reward Hypothesis](images/rl10.png)\n",
    "<br>\n",
    "3. Agent is also Penalized for left, right or any deviation from forward direction..\n",
    "\n",
    "    ![Reward Hypothesis](images/rl11.png)\n",
    "<br>\n",
    "4. And also Penalized if the Humanoid moved it's body away from the center of the track ..\n",
    "\n",
    "    ![Reward Hypothesis](images/rl12.png)\n",
    "<br>\n",
    "5. Lastly, it receives some positive reward if Humanoid has not fallen\n",
    "\n",
    "    ![Reward Hypothesis](images/rl13.png)\n",
    "<br>\n",
    "\n",
    "So while Agent is trying to maximize the reward, it has coincidentally learn to walk and not just walk, but walk faster, smoother and in right posture with the help of this simple yet but very effective reward formula.\n",
    "\n",
    "![Reward Hypothesis](images/rl14.png)\n",
    "\n",
    "Sama analogy applies to other use-cases though it may not be as simple and involves more complexity, however intuition is simple enough to understand and this makes this research so fascinating. Some other examples applying the same analogy are playing to learn Atari reward is the score, AlphaGo is winning # of games, Self-Driving Car to drive in lane avoiding accidents and so on ... \n",
    "\n",
    "If you'd like to learn more about the research that was done at DeepMind, please check out this [link](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/). The research paper can be accessed [here](https://arxiv.org/pdf/1707.02286.pdf). Also, check out this cool [video](https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be)!\n",
    "\n",
    "\n",
    "** How to piece this together **\n",
    "\n",
    "There are mutiple time-steps and objects involved like walking faster, smoother, stright etc. Though Agent can learn to maximize awards at each time step, it is important that it must learn to maximize the award at cumulative level. Example, if Agent see awards maximizing by walking faster, it may attempt to walk much faster even before smoothness and walking straight are achieved whcih may result into Episode not last very long impacting learning.\n",
    "\n",
    "One of the ways we can make Agent learn is keeping balacnce across the time-steps, example - we can restrict max speed having Agent walk slow , this way Agent can focus on other objectives and learn to perform better on those. Once these objectives are mastered, we can gradually open up the restrictions. Mathematically, we can breakdown rewards in zones like Past, Immediate and Future like below ..\n",
    "\n",
    "![Reward Hypothesis](images/rl15.png)\n",
    "\n",
    "Hence, if we see at time step t, and have Agent focus on $G{_t}$ which nothing but Rwards in future, we can say that Agent is attempting to maximize EXPECTED results ..\n",
    "\n",
    "We will go deeper in this subject, when we talk about Discounted Returns\n",
    "\n",
    "![Reward Hypothesis](images/rl16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Return\n",
    "\n",
    "What we mean by this is that Return's in near term are more valuable than the distant ones. ** This is specifically more important to Continuous Tasks as there is no end to them and near-term rewards are more relevant. ** In essence, any returns in future are all predictions, we know that predictions near term have higher accuracy then distant ones, hence if we look at future/expected reward $G{_t}$, which is cumulative of nth time steps, we know that distant the time step lesser accurate they are, hence we will compute new derivation of $G{_t}$ - in which we will give more weightage to near-term rewards and add a decay factor gama (values between 0 & 1) which will dilute the relevance of distant rewards.\n",
    "\n",
    "![Reward Hypothesis](images/rl17.png)\n",
    "\n",
    "In more generic sense,\n",
    "\n",
    "![Reward Hypothesis](images/rl18.png)\n",
    "\n",
    "Let's take two exammples of gama being 0 & 1 ..\n",
    "\n",
    "As we see, if GAMA is 1, the original reward equation remain un-discounted, i.e. no change at all.\n",
    "\n",
    "And if we make GAMA as 0, only the nearest reward holds hold good ..\n",
    "\n",
    "With this is we can define our intution that higher the value of GAMA give relevance to distant values and smaller the value of GAMA gives relevance to nearest values.\n",
    "\n",
    "![Reward Hypothesis](images/rl19.png)\n",
    "\n",
    "** Again, this is specifically more important to Continuous Tasks as there is no end to them and near-term rewards are more relevant. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![R](images/rl20.png)\n",
    "\n",
    "![H](images/rl21.png)\n",
    "\n",
    "![G](images/rl22.png)\n",
    "\n",
    "![B](images/rl23.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![B](images/rl24.png)\n",
    "![B](images/rl25.png)\n",
    "![B](images/rl26.png)\n",
    "![B](images/rl27.png)\n",
    "![B](images/rl28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP - We can now formally define MDP as ..\n",
    "\n",
    "![B](images/rl29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite MDPs\n",
    "\n",
    "OpenGym Envs\n",
    "https://github.com/openai/gym/wiki/Table-of-environments\n",
    "\n",
    "CartPole v0\n",
    "https://github.com/openai/gym/wiki/CartPole-v0\n",
    "\n",
    "![B](images/rl30.png)\n",
    "![B](images/rl31.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "![B](images/rl32.png)\n",
    "![B](images/rl33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Framework - The Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section we will look at Solution for MDP ..\n",
    "\n",
    "State or Env State what we have seen thus far in example is Forward Motion, but what if we encounter wall requiring humanoid to avoid collision, or gaps on floor requiring humanoid to jump and so on .. \n",
    "\n",
    "What this translate to is that State can vary and Actions for respective State has to in accordance to what that State demands. We define this as **Policies**.\n",
    "\n",
    "In more generic sense, we can say that we need to have mapping of Set of Actions for any given State.\n",
    "\n",
    "Policies are denoted by greek letter *Pi* \n",
    "\n",
    "**Deterministic Policy** When we have clear Action tied to State is called Deterministic.\n",
    "\n",
    "![B](images/rl34.png)\n",
    "\n",
    "**Stochastic Policy** When we have set of Actions with defined probablities in mapping, that means Agent will have choice of actions and each action has defined probability.\n",
    "\n",
    "![B](images/rl35.png)\n",
    "\n",
    "\n",
    "Let's take some examples to understand these better ..\n",
    "\n",
    "![B](images/rl36.png)\n",
    "\n",
    "Also, we can use same notation as Stochastic for Deterministic Policies as well, ie if we remove probablities with 0 and 1 in terms of output they would behave as Deterministic ..\n",
    "\n",
    "![B](images/rl37.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
